{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring trends in UK academia using PhD thesis metadata\n",
    "#### Markus Hauru, January 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 2009 the British Library has been operating its [Electronic Theses Online Service](https://ethos.bl.uk) or EThOS service, that keeps track of PhD theses accepted at UK higher education institutions. The Library also makes public a [dataset](https://data.bl.uk/ethos/) of metadata for all theses in their catalogue. Quoting the EThOS website,\n",
    "\n",
    ">The EThOS dataset lists virtually all UK doctoral theses ever awarded, some 500,000 dating back to 1787. All UK HE institutions are included, but we estimate records are missing for around 10,000 titles (2%).\n",
    "\n",
    "The latest version of the dataset is from March 2018. The data includes title, year, author, and institution for each thesis, as well as a link to a full record of each thesis, which may or may not include things like keywords or access to full texts, depending on the thesis.\n",
    "\n",
    "In this notebook I explore this dataset to identify historical trends in UK academia or other interesting features it may display. While an interesting project in itself, this notebook is also an exercise for myself, to learn some basic exploratory data science tools and techniques, like basics of pandas, some network analysis, and new visualization te."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, some imports of python libraries we'll be needing, and loading the data file into a pandas DataFrame. The script will automatically download the data file into the current working directory if it isn't there yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator as opr\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import community  # Network community finding\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns\n",
    "\n",
    "# for pretty-printing in notebooks\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pretty and interactive matplotlib plots in Jupyter Lab.\n",
    "%matplotlib widget\n",
    "# Set a consistent plotting style across the notebook using Seaborn.\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = \"EThOSCSV_201803.csv\"\n",
    "if not os.path.isfile(datafile):\n",
    "    # Download and/or unzip the data file from the EThOS website.\n",
    "    # We need a couple more imports for this.\n",
    "    import zipfile\n",
    "\n",
    "    datazip = \"EThOSCSV_201803.zip\"\n",
    "    if not os.path.isfile(datazip):\n",
    "        import requests\n",
    "\n",
    "        dataurl = \"https://data.bl.uk/ethos/EThOSCSV201803.zip\"\n",
    "        with requests.Session() as s:\n",
    "            headers = {\n",
    "                \"User-agent\": \"Mozilla/5.0 (X11; Linux i686; rv:64.0) Gecko/20100101 Firefox/64.0\"\n",
    "            }\n",
    "            r = s.get(dataurl, headers=headers)\n",
    "            with open(datazip, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "    with zipfile.ZipFile(datazip, \"r\") as z:\n",
    "        z.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(datafile, encoding=\"ISO-8859-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what the rows in the DataFrame look like, here are some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Institution</th>\n",
       "      <th>Date</th>\n",
       "      <th>Qualification</th>\n",
       "      <th>EThOS URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computation and measurement of turbulent flow ...</td>\n",
       "      <td>Loizou, Panos A.</td>\n",
       "      <td>University of Manchester</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prolactin and growth hormone secretion in norm...</td>\n",
       "      <td>Prescott, R. W. G.</td>\n",
       "      <td>University of Newcastle upon Tyne</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Influence of strain fields on flame propagation</td>\n",
       "      <td>Mendes-Lopes, J. M. C.</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Connectivity, flow and transport in network mo...</td>\n",
       "      <td>Robinson, Peter Clive</td>\n",
       "      <td>University of Oxford</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The theory and implementation of a high qualit...</td>\n",
       "      <td>Lower, K. N.</td>\n",
       "      <td>University of Bristol</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title                  Author  \\\n",
       "0  Computation and measurement of turbulent flow ...        Loizou, Panos A.   \n",
       "1  Prolactin and growth hormone secretion in norm...      Prescott, R. W. G.   \n",
       "2    Influence of strain fields on flame propagation  Mendes-Lopes, J. M. C.   \n",
       "3  Connectivity, flow and transport in network mo...   Robinson, Peter Clive   \n",
       "4  The theory and implementation of a high qualit...            Lower, K. N.   \n",
       "\n",
       "                         Institution    Date   Qualification  \\\n",
       "0           University of Manchester  1989.0  Thesis (Ph.D.)   \n",
       "1  University of Newcastle upon Tyne  1983.0  Thesis (Ph.D.)   \n",
       "2            University of Cambridge  1983.0  Thesis (Ph.D.)   \n",
       "3               University of Oxford  1984.0  Thesis (Ph.D.)   \n",
       "4              University of Bristol  1985.0  Thesis (Ph.D.)   \n",
       "\n",
       "                                           EThOS URL  \n",
       "0  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  \n",
       "1  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  \n",
       "2  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  \n",
       "3  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  \n",
       "4  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first observation to make is that the data is remarkably clean. There are a few NaNs that we need to drop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaNs: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows with NaNs: {}\".format(df.isnull().any(axis=1).sum()))\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but with that done, there's little else to do. The Qualification column holds some oddities and misunderstandings, and there are few suspiciously short titles (my favourite being the 1977 thesis called \"Beds\"), but other than that, things seem in order. Most notably, the Institution and Date fields, which we'll be getting a lot of mileage from, seem spotless, without even a single typoed university name. The Dates are all just years, so I convert them to an integer type and rename the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"] = df[\"Date\"].astype(np.int64)\n",
    "df.rename(columns={\"Date\": \"Year\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is of some concern are the 10,000 or so theses that the British Library estimates are missing from the data set. Nowadays that's about a year's worth of theses, but it's also roughly the total number of theses in the data from before 1956. Now clearly some of the missing ones are from the last few years: Even though the set is from 2018, the number of theses in the database drops sharply after 2015, even though it's been climbing up to that point. But there's probably a significant number missing from the early years as well, considering issues of record keeping. To illustrate the late and early parts of the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesiscounts_by_year = df.Year.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a57d91ac7ece4cb38f586b28a2a55120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(7, 3))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(thesiscounts_by_year.loc[2005:])\n",
    "plt.ylabel(\"Number of PhD theses\")\n",
    "plt.xlabel(\"Year\")\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(thesiscounts_by_year.loc[:1940])\n",
    "plt.xlabel(\"Year\")\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the end of WW1 the sample sizes for each year are clearly quite low, and based on this, as well as some odd features of the data pre-WW1, I've chosen to limit the time period to study to 1925-2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_cutoff = 2015\n",
    "early_cutoff = 1925\n",
    "thesiscounts_by_year = thesiscounts_by_year.loc[early_cutoff:late_cutoff]\n",
    "df = df[(early_cutoff <= df.Year) & (df.Year <= late_cutoff)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also drop the columns we won't be using for anything. Makes for nicer reading when we print out slices of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"Qualification\", \"EThOS URL\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of theses per year and institution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that basic setup out of the way, let's get into this.\n",
    "\n",
    "First, a basic look at the number of PhD theses accepted in the UK over our chosen time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "befc19435de64b01a64d589b67f6049b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(thesiscounts_by_year)\n",
    "ax.set_ylabel(\"Number of PhD theses.\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the logarithmic vertical axis. The two word summary here would clearly be \"exponential growth\". In 1925 a bit less than a hundred PhDs were awarded, and now we are approaching 20 000 per year. The most notable features are\n",
    "* a huge dip during WW2 in the middle of the otherwise steady exponential period from 1925 to 1960. (In the early-times plot a couple of cells above you can also see a dip during WW1, although the signal there is more noisy.) There's a corresponding bump a few years after WW2, presumably from people who postponed the start of their studies until after the war and are then graduating in large batches.\n",
    "* a period of even faster exponential growth in the 1960-1975 window. One explanation for this is baby boomers hitting typical PhD age (compare the above to a UK birth rate plot [here](https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/bulletins/birthsummarytablesenglandandwales/2017) and shift by 25 or so years), but probably the most important driver of this growth is the huge structural change in academia that we'll see in a moment.\n",
    "* a more moderate but steady pace from roughly 1975 onwards, that may be slowing down lately.\n",
    "\n",
    "Note that when I talk about rate or pace of growth here I'm talking in multiplicative terms, meaning for instance that since 1975 the number of new PhDs has been growing by a roughly constant _percentage_ per year, not a constant number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look at the contributions of different institutions to the total output of PhDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesiscounts_by_yearandinst = (\n",
    "    df.groupby([\"Institution\", \"Year\"])\n",
    "    .size()\n",
    "    .unstack(\"Institution\")\n",
    "    .fillna(0.0)\n",
    ")\n",
    "# Sort institutions based on PhD output in the last year of the time window.\n",
    "thesiscounts_by_yearandinst = thesiscounts_by_yearandinst.sort_values(\n",
    "    late_cutoff, axis=\"columns\", ascending=False\n",
    ")\n",
    "# Compute the proportion of PhDs produced by each institution by year.\n",
    "instratios_by_year = thesiscounts_by_yearandinst.divide(\n",
    "    thesiscounts_by_year, axis=\"index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bd94d736a241f9890add3b69e3d5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can't show the full legend with dozens of entries, so we restrict to a few\n",
    "# of the most important ones.\n",
    "num_legendentries = 10\n",
    "num_entries = len(instratios_by_year.columns)\n",
    "\n",
    "# Get the names and indices of the institutions that will cover the largest\n",
    "# area in this plot. Sort them by index.\n",
    "instratio_sums = instratios_by_year.sum().sort_values(ascending=False)\n",
    "top_insts = instratio_sums.index[:num_legendentries]\n",
    "top_inds = [list(instratios_by_year.columns).index(inst) for inst in top_insts]\n",
    "top_inds, top_insts = zip(*sorted(zip(top_inds, top_insts)))\n",
    "\n",
    "# Pick colors that highlight the entries of in the legend.\n",
    "legend_colors = sns.color_palette(\"dark\", n_colors=num_legendentries)\n",
    "# Color all the other entries with the same shade of gray, which we pick to be\n",
    "# one of the highlight colors desaturated.\n",
    "default_color = sns.set_hls_values(legend_colors[1], s=0.0)\n",
    "all_colors = [default_color] * num_entries\n",
    "# Put the highlight colors at the right positions in the list of all colors.\n",
    "for i, c in zip(top_inds, legend_colors):\n",
    "    all_colors[i] = c\n",
    "\n",
    "# Make an exception to the general style by giving this plot xticks.\n",
    "style_dict = {\"xtick.bottom\": True, \"xtick.direction\": \"inout\"}\n",
    "with sns.axes_style(style_dict):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    lines = plt.stackplot(\n",
    "        instratios_by_year.index,\n",
    "        instratios_by_year.values.T,\n",
    "        colors=all_colors,\n",
    "        lw=0.05,\n",
    "    )\n",
    "    plt.ylabel(\"Proportion of PhDs\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.xlim(early_cutoff, late_cutoff)\n",
    "    # Some fancy formatting of the legend. We show the most important legend\n",
    "    # entries in the same order that they appear in in the figure and position\n",
    "    # the legend out of the way.\n",
    "    top_lines = [lines[i] for i in top_inds]\n",
    "    plt.legend(\n",
    "        reversed(top_lines),\n",
    "        reversed(top_insts),\n",
    "        loc=\"lower left\",\n",
    "        bbox_to_anchor=(1.0, 0.0),\n",
    "    )\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the proportion of PhDs coming from different universities, each grey or colored band being one institution. The institutions are ordered by the proportion of PhDs they produced in the last year of the time window, 2015. The colors and the legend highlight a few of the most historically prominent instutions (the ones that get the largest area in the above plot).\n",
    "\n",
    "Here we can see the drastic change in the structure of higher education between roughly 1960 and 1975 that I mentioned above: There's an explosion in the number of PhDs coming from small universities. Until 1960 or so Oxford, Cambridge and Edinburgh produced 70-80% of PhDs in the country. By 1980 it was around 20%, and has stayed roughly constant since. They remain the largest PhD factories, but the field has been filled with dozens of smaller institutions. During the shift many new universities were founded (Sussex 1961, York 1963, Warwick 1965, ...), many others expanded greatly, and various colleges and other educational institutions were turned into universities. I'm not a historian, but for some background and context, check Wikipedia on for instance [University Grant Comission](https://en.wikipedia.org/wiki/University_Grants_Committee_(United_Kingdom)), [Robbins Report](https://en.wikipedia.org/wiki/Robbins_Report), and [Education Act of 1962](https://en.wikipedia.org/wiki/Education_Act_1962). I was previously aware that there was an expansion of higher education in many Western countries around this time, but the magnitude and speed of the shift in the above plot surprised me.\n",
    "\n",
    "There are also some interesting individual stories in this same plot. Oxford used to dominate its rival Cambridge with a doctoral output several times larger until the late 60s, after which the two have been roughly equal. Even Oxford was for a time ecplised by Edinburgh though, which in some years produced more than 40% of all PhDs in the UK. The historical importance of Scotland was further bolstered by Aberdeen, which used to be one of the major players until the 60s, but is now just another mid-sized entry. (It should be said though that I suspect that these early years may suffer from a significant number of missing theses, that may skew the data to some degree. The main qualitative statement probably still stands though.) Imperial has an interesting bulge of activity during the 1960s as it went through [a rapid expansion](https://en.wikipedia.org/wiki/Imperial_College_London#20th_century), a bit before many others followed suit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to underline the fact that the fall in the relative importance of the top few institutions is not at all due to shrinking on their part but merely the growth of others, here are the absolute numbers of PhDs produced by the 8 institutions that contributed the most in 1960."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca94d488340457296901e9e1b41cfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_insts = 4\n",
    "counts_top = thesiscounts_by_yearandinst.sort_values(\n",
    "    1960, axis=1, ascending=False\n",
    ")\n",
    "counts_top = counts_top.iloc[:, :num_insts]\n",
    "counts_top = (\n",
    "    counts_top.unstack().reset_index().rename(columns={0: \"Number of theses\"})\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    counts_top, col=\"Institution\", col_wrap=2, sharey=True, sharex=True\n",
    ")\n",
    "g.map(plt.plot, \"Year\", \"Number of theses\").set_titles(\"{col_name}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words, words, words..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've only looked at where and when PhD theses have been written. The dataset doesn't hold that much more information to toy with. It doesn't list departments or faculties, keywords or research fields, nor do I have easy access to full texts of the theses. But what it does have is the titles of these theses. Let's see what we can learn from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the title analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to looking at trends in titles, we need to spend a moment setting up some machinery that we can then milk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now thesis titles aren't exactly prose. They typically aren't full sentences and their grammatical forms vary wildly, which makes analysis of their linguistic structure tricky and, I would guess, in many cases a bit futile. Unlike with more structured text, almost as much information as in the title itself is contained in just a list of words appearing in the title. Lists of words are also easy to analyse, so we'll go with that.\n",
    "\n",
    "To start, we strip the titles of any punctation and so called stop words like articles and prepositions that don't tell us much, and make everything lower case. We could also do what's called stemming, and collapse for instance \"study\", \"studies\", \"studied\" and \"studying\" all into a single word. I choose not to do this because as we'll see below, different inflections of the same stem word sometimes appear in significantly different kinds of titles. Moreover, with academic vocabulary, doing this properly isn't straight-forward: the stemming algorithm should for instance know to combine \"phenomenon\" and \"phenomena\", but perhaps not \"phenomenal\" or \"phenomenology\", and certainly not \"phenolic\". A quick experiment that I did with one stemmer also suggested that the following results wouldn't be greatly affected.\n",
    "\n",
    "Another choice I make is to remove hyphens as unnecessary punctuation. Here you lose some and you win some. \"Post-modern\" should certainly be grouped together with \"postmodern\", but \"biodiversity-ecosystem\" (which appears in 5 titles) would be better split into two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitstr(s):\n",
    "    \"\"\" For a string, remove most punctuation, lower case, and split into\n",
    "    words. Return the unique words.\n",
    "    \"\"\"\n",
    "    puncts = \"!\\\"&'(),./:;<=>?[\\\\]`{|}-\"\n",
    "    words = s.lower().translate(str.maketrans(\"\", \"\", puncts)).split()\n",
    "    # Remove duplicates. We don't care if a word occurs multiple times in\n",
    "    # the same title.\n",
    "    words = tuple(set(words))\n",
    "    return words\n",
    "\n",
    "\n",
    "def filter_stopwords(words):\n",
    "    \"\"\" Take a list of strings, filter out prepositions, articles, and other\n",
    "    stop words.  We use a list of English stop words found here:\n",
    "    https://www.textfixer.com/tutorials/common-english-words.txt\n",
    "    \"\"\"\n",
    "    # We stop black (the code formatter) from automatically splitting this\n",
    "    # across a gazillion lines with the fmt: off/on.\n",
    "    # fmt: off\n",
    "    stops = [\"a\", \"able\", \"about\", \"across\", \"after\", \"all\", \"almost\", \"also\", \"am\", \"among\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"dear\", \"did\", \"do\", \"does\", \"either\", \"else\", \"ever\", \"every\", \"for\", \"from\", \"get\", \"got\", \"had\", \"has\", \"have\", \"he\", \"her\", \"hers\", \"him\", \"his\", \"how\", \"however\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"just\", \"least\", \"let\", \"like\", \"likely\", \"may\", \"me\", \"might\", \"most\", \"must\", \"my\", \"neither\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"often\", \"on\", \"only\", \"or\", \"other\", \"our\", \"own\", \"rather\", \"said\", \"say\", \"says\", \"she\", \"should\", \"since\", \"so\", \"some\", \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \"these\", \"they\", \"this\", \"tis\", \"to\", \"too\", \"twas\", \"us\", \"wants\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"would\", \"yet\", \"you\", \"your\"]\n",
    "    # fmt: on\n",
    "    return tuple(word for word in words if word not in stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Words\"] = df[\"Title\"].apply(splitstr).apply(filter_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the title analysis going, we want three different objects:\n",
    "* A DataFrame that lists the total number of titles in which each word appears.\n",
    "* The same thing but now per year. (We could do per institution as well, but let's leave that for later.)\n",
    "* A so called co-occurrence graph (or network), i.e. a weighted graph where nodes are words and edges tell us which words appear together in titles and how often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_by_year = (\n",
    "    df.groupby(\"Year\")[\"Words\"]\n",
    "    .apply(lambda x: pd.Series(np.concatenate(x.tolist())).value_counts())\n",
    "    .unstack(\"Year\")\n",
    "    .fillna(0.0)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_wordcounts = wordcounts_by_year.sum(axis=0)\n",
    "# Sort both total_wordcounts and wordcounts_by_year to have the most common\n",
    "# words first.\n",
    "order = (-total_wordcounts).argsort()\n",
    "total_wordcounts = total_wordcounts[order]\n",
    "allwords = total_wordcounts.index\n",
    "wordcounts_by_year = wordcounts_by_year.reindex(allwords, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the occurrences of each word per a 1000 thesis, in each given year, to measure the relative popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordratios_by_year = 1000 * wordcounts_by_year.divide(\n",
    "    thesiscounts_by_year, axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the co-occurrence graph requires some thought. First off, we've got around 190,000 distinct words in about 450,000 titles (academics like their jargon). However, only 20,000 or so of them appear in more than 10 thesis titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 190252. Number of theses: 451755. Number of words with more than 10 occurrences: 21584.\n"
     ]
    }
   ],
   "source": [
    "wordcount_cutoff = 10\n",
    "print(\n",
    "    \"Number of unique words: {}. Number of theses: {}. Number of words with more than {} occurrences: {}.\".format(\n",
    "        len(total_wordcounts),\n",
    "        len(df),\n",
    "        wordcount_cutoff,\n",
    "        (total_wordcounts > wordcount_cutoff).sum(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus I choose to restrict the graph to this subset of words, since it makes it computationally much lighter to handle, without causing much of a loss: Words that only occur in a handful of thesis titles probably wouldn't add much to the analysis anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainwords = allwords[total_wordcounts > wordcount_cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that when making word co-occurrence graphs of texts, co-occurrences are often given more weight if the words are next to each rather than just in the same sentence. I don't make this distinction since the titles are pretty short anyway, and word orders in titles can be unusual, making proximity less relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the graph/network, I use the [NetworkX](https://networkx.github.io/) package. It represents graphs as adjacency lists, which suits our quite sparse co-occurrence graph well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_graph = nx.Graph()\n",
    "cooc_graph.add_nodes_from(mainwords)\n",
    "for words in df[\"Words\"]:\n",
    "    words = tuple(filter(lambda w: w in mainwords, words))\n",
    "    # Loop over all pairs of two words `wi` and `wj` in `words`, but\n",
    "    # so that `(wi, wi)` are not included, and if `(wi, wj)` is included\n",
    "    # then `(wj, wi)` is not.\n",
    "    for i in range(len(words)):\n",
    "        wi = words[i]\n",
    "        # The second loop starts from i+1 to avoid double-counting.\n",
    "        for j in range(i + 1, len(words)):\n",
    "            wj = words[j]\n",
    "            if wi in cooc_graph[wj]:\n",
    "                # This edge already exists, increment the weight\n",
    "                cooc_graph[wi][wj][\"weight\"] += 1.0\n",
    "            else:\n",
    "                # This edge doesn't yet exist, create it.\n",
    "                cooc_graph.add_edge(wi, wj, weight=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have it now, the edge weights of the graph just count the number of theses in which both words appear. This isn't quite what we want, since it's dominated by commonly occuring words. There's several different ways we could normalize the weights. Dividing by the total appearance counts of the words (diagonal of the adjacency matrix) would make the weight of the edge between nodes A and B represent a conditional probability (or rather a frequency) P(A|B) of word A appearing in a title given that word B appears. We could also make a bidirectional graph that has both P(A|B) and P(B|A) stored for each edge. Or we could do a number of more symmetric normalizations that don't have as concrete an interpretation. A choice that I've found works well for what I want to do with this graph later is normalizing the edge between A and B by average of the frequencies of A and B. This is symmetric (the graph is undirected) and roughly speaking gives a large weight for edges that connect two words that appear roughly equally frequently and often together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w1, w2 in cooc_graph.edges:\n",
    "    avg_count = (total_wordcounts[w1] + total_wordcounts[w2]) / 2.0\n",
    "    cooc_graph[w1][w2][\"weight\"] /= avg_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a few convenient functions for studying individual words: Getting example thesis titles with a word init, getting related words, and outputting a summary of a word (popularity plot, related words, examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_titles(word, n=1):\n",
    "    \"\"\" For a given word `w`, return a tuple of `n` randomly picked thesis\n",
    "    titles that have `w` in it.\n",
    "    \"\"\"\n",
    "    titles_with_word = df[df[\"Words\"].apply(lambda x: word in x)][\"Title\"]\n",
    "    n = min(n, len(titles_with_word))\n",
    "    examples = tuple(titles_with_word.sample(n).values)\n",
    "    return examples\n",
    "\n",
    "\n",
    "def get_related_words(w, n=5, weights=True):\n",
    "    \"\"\" For a given word `w`, get the `n` words with heaviest edges connected\n",
    "    to `w` in the co-occurrence graph.  Returns a list of tuples\n",
    "    `(neighbour_word, weight)`, sorted by `weight`, or alternatively just a\n",
    "    list of `neighbour_word`s if `weights=False`.\n",
    "    \"\"\"\n",
    "    if w not in cooc_graph:\n",
    "        # This word is not in the co-occurrence network.\n",
    "        return ()\n",
    "    node = cooc_graph[w]\n",
    "    neighbourlist = ((k, v[\"weight\"]) for k, v in node.items())\n",
    "    neighbourlist = sorted(neighbourlist, key=opr.itemgetter(1), reverse=True)\n",
    "    neighbourlist = neighbourlist[:n]\n",
    "    if not weights:\n",
    "        neighbourlist = [w[0] for w in neighbourlist]\n",
    "    return neighbourlist\n",
    "\n",
    "\n",
    "def output_word_summary(w, n_related=3, n_examples=3, plot=True):\n",
    "    \"\"\" Print and plot (if `plot=True`) a summary of data for the word `w`.\n",
    "    \"\"\"\n",
    "    print(\"Word: {}\".format(w))\n",
    "    if n_related > 0:\n",
    "        print(\"Related words:\")\n",
    "        for neighbour in get_related_words(w, n=n_related):\n",
    "            print(\" {}: {}\".format(*neighbour))\n",
    "    if n_examples > 0:\n",
    "        print(\"Example titles:\")\n",
    "        for title in get_example_titles(w, n=n_examples):\n",
    "            print(\" {}\".format(title))\n",
    "    if plot:\n",
    "        fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "        ax1.plot(wordcounts_by_year[w], color=\"black\")\n",
    "        title = \"Number and proportion of theses\\nwith '{}' in the title\".format(\n",
    "            w\n",
    "        )\n",
    "        ax1.set_title(title)\n",
    "        ax1.set_ylabel(\"Absolute number (black)\".format(w))\n",
    "        ax1.set_xlabel(\"Year\")\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(wordratios_by_year[w], color=\"darkred\")\n",
    "        ax2.set_ylabel(\"Per 1000 theses (red)\".format(w))\n",
    "        fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends in individual words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all that set-up out of the way, let's see if these titles hold some interesting. First off, what are the most commonly occuring words in thesis titles, and how does their popularity vary over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a3bc24e9a2542fb8b51c73c9027fe03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Word: study\n",
      "Related words:\n",
      " case: 0.2097815470376635\n",
      " comparative: 0.1153317337503033\n",
      " development: 0.05929317390646284\n",
      "Example titles:\n",
      " The state scale of dissociation : development, psychometric  validation, and application in a study of concurrent electro-encephalographic correlates\n",
      " A study on the localisation and function of the human chloride channel ClC-3\n",
      " Mothers in two contemporary women's autobiographies : a linguistic study of voice, character and relationship\n",
      "--------------------------------------------------\n",
      "Word: studies\n",
      "Related words:\n",
      " structural: 0.09210299541553205\n",
      " synthesis: 0.050235956766631146\n",
      " synthetic: 0.048787855902122194\n",
      "Example titles:\n",
      " Studies in corporate risk management\n",
      " Laser spectroscopy, matrix isolation and ab initio studies of free radicals and weakly bound complexes\n",
      " Studies in gastric mucins\n",
      "--------------------------------------------------\n",
      "Word: development\n",
      "Related words:\n",
      " system: 0.06787693205016039\n",
      " application: 0.06536252134761683\n",
      " role: 0.0598641998380365\n",
      "Example titles:\n",
      " The development of the auditory space map in the superior colliculus : explorations around a Hebbian hypothesis\n",
      " Regional industrial development : with specific reference to the Foyle Basin\n",
      " Development of a growth factor delivery system\n",
      "--------------------------------------------------\n",
      "Word: analysis\n",
      "Related words:\n",
      " using: 0.06964454230890217\n",
      " data: 0.06052919308987535\n",
      " functional: 0.053515590100955954\n",
      "Example titles:\n",
      " Studies in the language of Oppian of Cilicia : an analysis of the new formations in the Halieutica\n",
      " Development and evaluation of an ion induced X-ray emission system for the analysis of light and medium weight elements\n",
      " Thermal analysis studies on cadmium oxalate and related compounds\n"
     ]
    }
   ],
   "source": [
    "num_topwords = 4\n",
    "topwords = allwords[:num_topwords]\n",
    "topword_ratios = wordratios_by_year.loc[:, topwords]\n",
    "topword_ratios = (\n",
    "    topword_ratios.unstack()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"Occurrences per 1000 theses\", \"level_0\": \"Word\"})\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    topword_ratios, col=\"Word\", col_wrap=2, sharex=True, sharey=True,\n",
    ")\n",
    "g.map(plt.plot, \"Year\", \"Occurrences per 1000 theses\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "for w in topwords:\n",
    "    print(\"-\" * 50)\n",
    "    output_word_summary(w, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing too exciting here: Generic terms that appear in all kinds of titles. Both \"study\" and \"studies\" have clearly fallen into relative disuse since the 70s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more interesting than the most popular words, are those whose popularity has been fleeting. One could get at this in a million different ways, but here's one that I found interesting. For each decade, find the top 3 words whose popularity that decade was the most disproportionately large compared to their popularity over all time. In other words, rank words by number of occurences in a given decade, divided by total number of occurences. To not have this be dominated by words occuring in only a handful of theses, further restrict to words with at least 100 occurences in total. I'll also print for each word some of the commonly co-occurring words, to give some context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1930s:\n",
      "notes            ['introduction', 'glossary', 'edited']\n",
      "solvents         ['eutectic', 'nonpolar', 'nonaqueous']\n",
      "prognosis        ['diagnosis', 'aetiology', 'infarction']\n",
      "-------------------------------------------------------------------------------\n",
      "1940s:\n",
      "substances       ['humic', 'pectic', 'psychoactive']\n",
      "constitution     ['alloys', 'constitutional', 'discursive']\n",
      "cases            ['hundred', 'review', 'report']\n",
      "-------------------------------------------------------------------------------\n",
      "1950s:\n",
      "radioactive      ['disposal', 'tracer', 'waste']\n",
      "substances       ['humic', 'pectic', 'psychoactive']\n",
      "constituents     ['nitrogenous', 'milk', 'leaves']\n",
      "-------------------------------------------------------------------------------\n",
      "1960s:\n",
      "elementary       ['particles', 'particle', 'physics']\n",
      "polysaccharides  ['algal', 'pectic', 'enzymic']\n",
      "bubble           ['chamber', 'columns', 'coalescence']\n",
      "-------------------------------------------------------------------------------\n",
      "1970s:\n",
      "gevc             ['kp', '10', '16']\n",
      "prostaglandins   ['parturition', 'leucocytes', 'menstruation']\n",
      "ultrastructure   ['cytochemistry', 'histochemistry', 'cytology']\n",
      "-------------------------------------------------------------------------------\n",
      "1980s:\n",
      "microprocessor   ['pwm', 'inverter', 'drive']\n",
      "monoclonal       ['antibodies', 'antibody', 'antigens']\n",
      "diagenesis       ['sedimentology', 'sandstones', 'jurassic']\n",
      "-------------------------------------------------------------------------------\n",
      "1990s:\n",
      "objectoriented   ['database', 'databases', 'query']\n",
      "atm              ['traffic', 'congestion', 'switch']\n",
      "lipoprotein      ['lipase', 'density', 'cholesterol']\n",
      "-------------------------------------------------------------------------------\n",
      "2000s:\n",
      "globalisation    ['era', 'firmlevel', 'olympic']\n",
      "internet         ['banking', 'online', 'protocol']\n",
      "ict              ['adoption', 'teachers', 'technology']\n",
      "-------------------------------------------------------------------------------\n",
      "2010s:\n",
      "graphene         ['epitaxial', 'nanoelectronics', 'nanotubes']\n",
      "mindfulness      ['selfcompassion', 'meditation', 'dispositional']\n",
      "resilience       ['socialecological', 'disaster', 'vulnerability']\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_topwords = 3\n",
    "occurrence_cutoff = 100\n",
    "notrare_words = total_wordcounts > occurrence_cutoff\n",
    "for start_year in range(1930, 2020, 10):\n",
    "    end_year = start_year + 10\n",
    "    print(\"{}s:\".format(start_year))\n",
    "    decade_top_words = (\n",
    "        wordcounts_by_year.loc[start_year:end_year, notrare_words].sum()\n",
    "        / total_wordcounts\n",
    "    ).sort_values(ascending=False)\n",
    "    for w in decade_top_words.index[:num_topwords]:\n",
    "        neighbours = get_related_words(w, n=3, weights=False)\n",
    "        print(\"{:15}  {}\".format(w, neighbours))\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 30s, 40s, and 50s look quite similar, with pretty generic terminology, especially science terminology, trending high, but from that point on things get more specialised and interesting. Plenty of particle physics in the 60s and 70s (a bubble chamber is a type of particle physics experiment and GeV/c, gigaelectronvolts per speed of light, is a a unit for momentum in particle physics), when new particles were constantly being discovered. The 80s, 90s and 2000s have a lot of computer science and communication technology words in the lead (ATM in most cases stands for asynchronous transfer mode), giving way to \"mindfulness\" and \"resilience\" in the last decade. They also highlight the relative lack of humanities and social sciences terminology in the list, but we'll come back that later.\n",
    "\n",
    "The prospective rabbit holes here are endless, but just for highlighting, here's a bit more detail on a few select words from the above list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f2285d0081487ba1aa2e3afb642212",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Word: bubble\n",
      "Related words:\n",
      " chamber: 0.23267326732673269\n",
      " columns: 0.0691358024691358\n",
      " coalescence: 0.060836501901140684\n",
      "Example titles:\n",
      " A study of strange particle production in high energy antiproton-proton collisions in a liquid hydrogen bubble chamber\n",
      " A study of foam stability and the kinetics of bubble growth in glass at high temperature\n",
      " The residual-free bubble method for problems with multiple scales\n",
      "--------------------------------------------------\n",
      "Word: diagenesis\n",
      "Related words:\n",
      " sedimentology: 0.14255765199161424\n",
      " sandstones: 0.10810810810810811\n",
      " jurassic: 0.07514450867052024\n",
      "Example titles:\n",
      " Diagenesis related to thrust sheet emplacement : Tellian Atlas, northern Algeria\n",
      " The dolomitisation and related diagenesis of Dinantian limestones, Derbyshire\n",
      " The sedimentary environments and diagenesis of Namurian reservoir sandstones, Trumfleet Field, Yorkshire\n",
      "--------------------------------------------------\n",
      "Word: globalisation\n",
      "Related words:\n",
      " era: 0.04203152364273205\n",
      " firmlevel: 0.03319502074688797\n",
      " olympic: 0.028169014084507043\n",
      "Example titles:\n",
      " Union strategies in the era of globalisation : case studies from Chile's large-scale copper mining sector (1982-2009)\n",
      " Globalisation, the European Union and Turkey : rethinking the struggle over hegemony\n",
      " Italian football in an era of globalisation : neo-patrimony, new localism and decline\n",
      "--------------------------------------------------\n",
      "Word: mindfulness\n",
      "Related words:\n",
      " selfcompassion: 0.08974358974358974\n",
      " meditation: 0.06666666666666667\n",
      " dispositional: 0.03125\n",
      "Example titles:\n",
      " Diabetes : the benefits of mindfulness interventions and the role of cognitive flexibility\n",
      " Children's experiences of learning mindfulness to help develop their attentional skills\n",
      " Mindfulness during pregnancy : an evaluation of mindfulness and negative mood over the perinatal period\n"
     ]
    }
   ],
   "source": [
    "selectwords = [\"bubble\", \"diagenesis\", \"globalisation\", \"mindfulness\"]\n",
    "selectword_ratios = wordratios_by_year.loc[:, selectwords]\n",
    "selectword_ratios = (\n",
    "    selectword_ratios.unstack()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"Occurrences per 1000 theses\", \"level_0\": \"Word\"})\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    selectword_ratios, col=\"Word\", col_wrap=2, sharex=True, sharey=False,\n",
    ")\n",
    "g.map(plt.plot, \"Year\", \"Occurrences per 1000 theses\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "for w in selectwords:\n",
    "    print(\"-\" * 50)\n",
    "    output_word_summary(w, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 points go to Humanities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual words like those above tell us stories of individual scientific discoveries and hot topics, but they have their limits if we want to study larger academic trends. However, one would naturally expect the co-occurrences of words in titles to follow patterns, where words related to fields and subfields would often appear together. Our next goal is to see if this indeed happens, and assuming it does (yes, it does), use it to\n",
    "* identify what are the academic field distinctions that the co-occurence graph holds.\n",
    "* analyse how the popularities of these different fields have varied.\n",
    "\n",
    "So we want to find groups of words that typically appear together. This could be called clustering, but in the context of networks/graphs like our co-occurrence graph, it usually goes by the name of community structure. There's plenty of research done on developing algorithms for identifying communities. We use below one quite well-known one, the [Louvain algorithm](https://arxiv.org/abs/0803.0476), which is a heuristic algorithm based on optimizing the modularity of the graph, i.e. minimizing the weight of edges connecting different communities and maximizing the weight of edges internal to communities. I tried a few other methods as well, most notably label-propagation and stochastic block models, but at least with the parameters that I tried they seemed to produce communities that were either very small or did not match well my human intuition of which words I would expect to be related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this notebook yourself, you can safely go make a cup of tea at this point. The community finding takes around  minutes on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community.best_partition(cooc_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community.best_partition returns a dictionary of {word: community_label}.\n",
    "# Lets turn that into a DataFrame and extract the lists of words belonging to each community.\n",
    "community_labels = pd.DataFrame(\n",
    "    {\"Label\": tuple(partition.values())},\n",
    "    index=partition.keys(),\n",
    "    dtype=\"category\",\n",
    ")\n",
    "communities = tuple(\n",
    "    map(tuple, community_labels.groupby(\"Label\").groups.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of communities: 17\n",
      "Numbers of words in the communities:\n",
      "[3561, 5806, 4129, 5086, 2951, 6, 11, 4, 3, 2, 5, 5, 2, 2, 6, 3, 2]\n",
      "Some example words from each community:\n",
      "('study', 'development', 'analysis', 'investigation', 'role', 'case', 'social', 'between', 'reference', 'use', 'behaviour', 'approach', 'theory', 'management', 'new', 'model', 'aspects', 'performance', 'education', 'factors', 'impact', 'evaluation', 'learning', 'english', 'health', 'policy', 'british', 'during', 'towards', 'special', 'influence', 'uk', 'change', 'practice', 'england', 'assessment', 'children', 'production', 'process', 'processes')\n",
      "\n",
      "('studies', 'using', 'systems', 'control', 'modelling', 'design', 'synthesis', 'system', 'properties', 'structure', 'application', 'effect', 'applications', 'high', 'models', 'based', 'methods', 'dynamics', 'structural', 'techniques', 'flow', 'data', 'experimental', 'networks', 'metal', 'power', 'problems', 'reactions', 'energy', 'processing', 'structures', 'materials', 'surface', 'related', 'compounds', 'water', 'measurement', 'optical', 'magnetic', 'complexes')\n",
      "\n",
      "('effects', 'human', 'characterisation', 'novel', 'molecular', 'cell', 'cells', 'growth', 'disease', 'regulation', 'function', 'interactions', 'protein', 'mechanisms', 'genetic', 'response', 'gene', 'cancer', 'expression', 'functional', 'potential', 'activity', 'metabolism', 'responses', 'proteins', 'identification', 'patients', 'complex', 'rat', 'acid', 'resistance', 'dna', 'associated', 'biological', 'changes', 'receptor', 'vitro', 'virus', 'type', 'signalling')\n",
      "\n",
      "('early', 'history', 'century', 'contemporary', 'late', 'art', 'modern', 'war', 'music', 'literature', 'representation', 'writing', 'fiction', 'historical', 'works', 'poetry', 'american', 'french', 'representations', 'interpretation', 'de', 'religious', 'john', 'thought', 'literary', 'greek', 'church', 'significance', 'period', 'theology', 'narrative', 'german', 'place', 'irish', 'philosophy', 'roman', 'christian', 'old', 'translation', 'france')\n",
      "\n",
      "('evolution', 'l', 'distribution', 'ecology', 'scotland', 'central', 'species', 'north', 'population', 'plant', 'biology', 'soil', 'variation', 'southern', 'region', 'marine', 'western', 'composition', 'sea', 'area', 'nitrogen', 'climate', 'plants', 'conservation', 'eastern', 'populations', 'diversity', 'soils', 'ecological', 'certain', 'morphology', 'temporal', 'landscape', 'forest', 'upper', 'lower', 'physiology', 'river', 'impacts', 'microbial')\n",
      "\n",
      "('cassava', 'phenylalanine', 'manihot', 'esculenta', 'ammonialyase', 'crantz')\n",
      "\n",
      "('bath', 'monooxygenase', 'oxidising', 'capsulatus', 'methylotrophic', 'obligate', 'methylococcus', 'methanotrophs', 'ob3b', 'methylosinus', 'trichosporium')\n",
      "\n",
      "('es', 'dar', 'frustration', 'salaam')\n",
      "\n",
      "('implantable', 'cardioverter', 'defibrillator')\n",
      "\n",
      "('script', 'cursive')\n",
      "\n",
      "('described', 'suboptimal', 'fedbatch', 'exothermic', 'chosen')\n",
      "\n",
      "('ho', 'chi', 'diarrhoeal', 'canals', 'minh')\n",
      "\n",
      "('faso', 'burkina')\n",
      "\n",
      "('jumping', 'conclusions')\n",
      "\n",
      "('lao', 'catching', 'governor', 'hydro', 'pdr', 'mini')\n",
      "\n",
      "('lightinduced', 'myxococcus', 'xanthus')\n",
      "\n",
      "('legionella', 'pneumophila')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of communities: {}\".format(len(communities)))\n",
    "print(\"Numbers of words in the communities:\")\n",
    "print([len(c) for c in communities])\n",
    "print(\"Some example words from each community:\")\n",
    "num_example_words = 40\n",
    "for c in communities:\n",
    "    print(c[:num_example_words], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So according to the Louvain algorithm, our co-occurrence graph has 19 communities, out of 5 are non-tiny. We'll just discard the 14 small ones as uninteresting (not every word needs to be in a community, recall that our graph only includes the ~20,000 most common words anyway), and focus on the 5 big ones.\n",
    "\n",
    "Gladly, they make quite good sense to human intuition. The first one has a lot of social sciences vocabulary in it, we'll call it Social; the second one is clearly hard sciences, maths, and engineering or just Science for short; the second is medicine and biochemistry, aka Biomed; the fourth one clearly Humanities; and the fifth has a theme of ecology and geography, and we'll call it Eco/Geo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point a note about stability of these communities is in order. I've been running the same community finding on graphs made with only subsets of the data set, leaving out some of the theses, and also tried including more or less words in the co-occurrence graph (remember we picked the arbitrary cutoff of only including words with at least 10 occurrences). The main communities fortunately appear quite stable under such perturbations. Including some what less words or less theses sometimes makes Humanities merge with Social sciences, but otherwise the communities stay roughly as they are. Going the other way, including more rare words in the graph may for instance sometimes cause particle physics to separate from the rest of Science. These kinds of granularity differences, of whether a field splits into a further subfields or merges with its academic neighbour, is quite natural, and doesn't in my view undermine the analysis in any significant way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, what we would like to do is assign to each word and thesis title a score or label, that would tell us roughly which field(s) it belongs in. The naive way would be to just use to label each word with its respective community, and count how many humanities words appear in a given title, etc. This doesn't seem quite fair though. Clearly some words are in some sense more \"deeply\" in each community. For instance the word \"theory\" gets grouped into social sciences, but obviously it occurs in other contexts as well, unlike the word \"policy\", which is pretty dead giveaway. To account for this effect, we'll give each word a score for how strongly they are connected to each community. This score starts out being 1 for the community the word belongs in and 0 for the others, but we add to it the total weight of edges connecting this word to words in a given community. So a word that is in the Humanities community and co-occurs mostly with other Humanities words gets a high Humanities-score, whereas a word that co-occurs with words from several different communities will have significant scores for all of them. Finally, we'll normalize the scores by the total sum within a field, so for instance the Social score of each word will be divided by the sum of Social scores of all words. This accounts for the fact that some fields include more words, and these words may be more strongly connected in our co-occurrence graph. We'll also multiply the resulting scores by 10,000, just to produce more human-readable numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, use some key words in each community to anchor the names of the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_keywords = {\n",
    "    \"education\": \"Social\",\n",
    "    \"cell\": \"Biomed\",\n",
    "    \"magnetism\": \"Science\",\n",
    "    \"ecology\": \"Eco/Geo\",\n",
    "    \"philosophy\": \"Humanities\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First give each word a score of 1.0 for the community it belongs in according to the Louvain classification. Drop all the small communities we don't care about, and name the columns of the DataFrame with the names given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordscores = pd.get_dummies(community_labels).astype(np.float_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns whose names are in field_keywords.\n",
    "for word, field in field_keywords.items():\n",
    "    current_label = wordscores.loc[word, :].idxmax()\n",
    "    wordscores.rename(columns={current_label: field}, inplace=True)\n",
    "# Drop all other columns.\n",
    "for c in wordscores.columns:\n",
    "    if not c in field_keywords.values():\n",
    "        wordscores.drop(c, axis=1, inplace=True)\n",
    "fieldnames = wordscores.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then add to the scores the weights of the edges connecting each word to words of different communities. This is easy to do with a matrix product with the adjacency matrix of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So to get the scores we want, we need to take the matrix product of the\n",
    "# adjacency matrix of the co-occurrence graph with the current score matrix,\n",
    "# wordscores.values. The graph is way too big to build the whole adjacency\n",
    "# matrix as a dense matrix, but luckily NetworkX has us covered, with the\n",
    "# possibility of creating a scipy sparse matrix instead. However, the pandas\n",
    "# dot function for matrix products insists in converting everything to a dense\n",
    "# matrix to do the product, so we'll have to manually work with the\n",
    "# scipy.sparse matrix instead of a DataFrame. It's not too bad though.\n",
    "adjacency_matrix = nx.to_scipy_sparse_matrix(\n",
    "    cooc_graph, nodelist=wordscores.index, format=\"csr\"\n",
    ")\n",
    "for c in fieldnames:\n",
    "    column_vec = wordscores[c].to_numpy()\n",
    "    scores = adjacency_matrix.dot(column_vec)\n",
    "    scores_series = pd.Series(scores, index=wordscores.index)\n",
    "    wordscores[c] += scores_series\n",
    "del adjacency_matrix  # Release the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, normalize the scores, and give all the words we did not include in our graph (the ones with fewer than 10 occurrences) a score of 0.0 for everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordscores *= 10000 / wordscores.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordscores = wordscores.reindex(allwords, fill_value=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the scoring system makes sense, let's check the scores for the top words for each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words with highest scores for Social:\n",
      "               Social   Science    Biomed  Humanities   Eco/Geo\n",
      "case         6.775540  0.648267  0.330254    1.101521  0.820979\n",
      "education    6.738411  0.338990  0.212264    1.083079  0.271432\n",
      "experiences  6.592405  0.291959  0.554990    0.753946  0.180923\n",
      "school       6.534148  0.338828  0.290503    0.834029  0.253131\n",
      "students     6.476771  0.362921  0.218035    0.768205  0.179574\n",
      "health       6.324062  0.549101  1.054213    0.533970  0.497262\n",
      "\n",
      "Words with highest scores for Science:\n",
      "            Social   Science    Biomed  Humanities   Eco/Geo\n",
      "high      1.405260  4.499568  0.776721    0.203490  0.573054\n",
      "using     2.195163  4.477952  1.582520    0.253442  0.823530\n",
      "laser     0.353529  4.308319  0.422768    0.054313  0.174001\n",
      "flow      0.780519  4.205394  0.735368    0.096122  0.737094\n",
      "optical   0.434796  4.187725  0.417813    0.064055  0.184917\n",
      "detector  0.241209  4.167331  0.208750    0.115004  0.091805\n",
      "\n",
      "Words with highest scores for Biomed:\n",
      "              Social   Science    Biomed  Humanities   Eco/Geo\n",
      "cells       0.684412  1.178612  6.273689    0.069176  0.357731\n",
      "cell        0.864979  1.172025  6.098188    0.080546  0.506021\n",
      "receptor    0.453661  0.490607  6.023090    0.044532  0.214090\n",
      "expression  0.728418  0.535393  5.817133    0.240766  0.634580\n",
      "rat         0.547202  0.614644  5.763338    0.056791  0.347501\n",
      "gene        0.695654  0.612467  5.666892    0.080381  0.695220\n",
      "\n",
      "Words with highest scores for Humanities:\n",
      "             Social   Science    Biomed  Humanities   Eco/Geo\n",
      "century    2.263923  0.271204  0.175152    4.797174  0.523751\n",
      "centuries  0.952846  0.198722  0.105385    4.614226  0.472073\n",
      "john       0.947633  0.181905  0.114063    4.356534  0.245045\n",
      "theology   1.086491  0.174233  0.163153    4.309942  0.123234\n",
      "works      1.215341  0.290461  0.165828    4.128450  0.253888\n",
      "edition    0.469748  0.148402  0.161474    4.097790  0.180027\n",
      "\n",
      "Words with highest scores for Eco/Geo:\n",
      "            Social   Science    Biomed  Humanities   Eco/Geo\n",
      "l         0.826905  0.542279  1.406821    0.192037  7.992934\n",
      "basin     0.863159  0.482790  0.151067    0.436617  6.344290\n",
      "salmo     0.264088  0.225159  0.773632    0.140045  5.994034\n",
      "atlantic  0.707755  0.592436  0.539149    0.412391  5.940677\n",
      "ecology   1.038833  0.301024  0.542660    0.269917  5.933348\n",
      "trout     0.264230  0.231399  1.104970    0.103421  5.784830\n"
     ]
    }
   ],
   "source": [
    "num_topwords = 6\n",
    "for c in fieldnames:\n",
    "    print(\"\\nWords with highest scores for {}:\".format(c))\n",
    "    print(wordscores.sort_values(c, ascending=False)[:num_topwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a few funny ones, like \"L\" being the top word in Eco/Geo because it often stands for Carl Linnaeus, the founder of modern taxonomy, in scientific names of species, but overall the scores seem pretty reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, computing the scores for each thesis title, by just taking the average of the scores of individual words in the title. This takes a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing title scores for Social.\n",
      "Computing title scores for Science.\n",
      "Computing title scores for Biomed.\n",
      "Computing title scores for Humanities.\n",
      "Computing title scores for Eco/Geo.\n"
     ]
    }
   ],
   "source": [
    "for c in fieldnames:\n",
    "    print(\"Computing title scores for {}.\".format(c))\n",
    "    # Interestingly the python sum function is significantly faster here than a\n",
    "    # sub-DataFrame.sum(). Probably due to allocation of temporaries.\n",
    "    df[c] = df[\"Words\"].apply(\n",
    "        lambda ws: 0.0  # Special case to avoid divison by zero.\n",
    "        if not ws\n",
    "        else sum(wordscores.loc[w, c] for w in ws) / len(ws)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trends in academic fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With each thesis scored on all the academic fields, it's time to see what this data tells us. Let's first see how the scores are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfc46452a92c40f592de5d2687c79b0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = 50  # For histograms\n",
    "maxscore = 5  # Max score to show on the plot\n",
    "\n",
    "\n",
    "def custom_light_palette(color):\n",
    "    \"\"\" A color palette a bit like seaborn.light_palette, but starts from pure\n",
    "    white and goes to darker shades.\n",
    "    \"\"\"\n",
    "    start = sns.set_hls_values(color, l=1.0)\n",
    "    end = sns.set_hls_values(color, l=0.1)\n",
    "    colors = [start, color, end]\n",
    "    return sns.blend_palette(colors, as_cmap=True)\n",
    "\n",
    "\n",
    "def hist2d(x, y, color, **kwargs):\n",
    "    cmap = custom_light_palette(color)\n",
    "    rng = [[0, maxscore]] * 2\n",
    "    plt.hist2d(x, y, bins=bins, range=rng, cmap=cmap, **kwargs)\n",
    "\n",
    "\n",
    "def hist(x, color, **kwargs):\n",
    "    rng = [0, maxscore]\n",
    "    plt.hist(x, lw=0.0, bins=bins, range=rng, **kwargs)\n",
    "\n",
    "\n",
    "def corr(x, y, color, **kwargs):\n",
    "    \"\"\" Given two Series, compute their correlation and produce a pyplot plot\n",
    "    that just has that number as text in the middle, and nothing else.\n",
    "    \"\"\"\n",
    "    corr = x.corr(y)\n",
    "    corr_str = \"{:.3f}\".format(corr)\n",
    "    plt.plot()\n",
    "    ax = plt.gca()\n",
    "    plt.text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        corr_str,\n",
    "        horizontalalignment=\"center\",\n",
    "        verticalalignment=\"center\",\n",
    "        transform=ax.transAxes,\n",
    "    )\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", length=0)\n",
    "\n",
    "\n",
    "sns.set_style(\"white\")  # We use a different style for this plot than most.\n",
    "g = sns.PairGrid(df[fieldnames], height=1.5)\n",
    "g.map_upper(corr)\n",
    "g.map_lower(hist2d)\n",
    "g.map_diag(hist)\n",
    "g.set(ylim=(0, maxscore), xlim=(0, maxscore))\n",
    "ticks = [0.0, maxscore / 2, maxscore]\n",
    "g.set(xticks=ticks, yticks=ticks)\n",
    "sns.set_style(\"darkgrid\")  # Back to the usual style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here on the diagonal are histograms showing the how many theses get which score, for each of the fields. Above the diagonal, the single numbers shown there are correlation coefficients between the field scores. For instance since the second row of plots is for Science and the fourth column is for Humanities, the number at their intersection, around -0.5, is the correlation coefficient between Science and Humanities scores. Mirroring these correlations, below the diagonal we have 2D histograms showing the distribution of theses for each pair of two fields.\n",
    "\n",
    "Looking first at the 1D histograms on the diagonal, note that the first peak in each of them is not the interesting part. Since we smudged the boundaries of fields by adding neighbouring edge weights when scoring words, most words have a non-zero score for all the fields. The first peaks then represent a sort of baseline value, of what's a usual score for a thesis that isn't really in the given field. The interesting bits are the fat tails, the bulges, around scores of 2 and more. They represent the theses that scores unusually high for a given field.\n",
    "\n",
    "The next observation is that the distributions for different fields are very different. This is natural, given that some (e.g. Eco/Geo) are much more specific than others (e.g. Science). This does mean though that, despite our best attempts to normalize the scores meaningfully, we probably shouldn't assign much meaning to statements like \"this thesis has a much higher Humanities than Social score\", but rather concentrate on correlations and trends that are independent of the overall scale of the scores.\n",
    "\n",
    "The correlations tell us a lot about how distinct the different fields are. The highest positive correlation is between Humanities and Social sciences, which isn't very surprising, and goes together with my observation that sometimes the two fields fuse in the community finding analysis if a smaller data set is used. They both are heavily anti-correlated with Science and Biomed, again matching intuitive expectation. The one thing that somewhat surprises me is that Science, Biomed, and Eco/Geo are virtually independent of each other: I would have guessed topics like chemistry and population biology to cause some mixing between them. This is good though, since it makes the 5-way scoring more meaningful, when the scores aren't predictable from each other. The 2D histograms below the diagonal provide a more detailed visual picture of what is summarised by the correlation coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being pretty happy with our scoring method, lets see what trends we can detect over time and institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, here are the mean scores of theses published in each year, over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc45b7a918084f728971206f44500b70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meanscores_by_year = df.groupby([\"Year\"]).mean().fillna(0.0)\n",
    "meanscores_by_year = (\n",
    "    meanscores_by_year.unstack()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"Mean score\", \"level_0\": \"Field\"})\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    meanscores_by_year,\n",
    "    col=\"Field\",\n",
    "    col_wrap=2,\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    height=2.0,\n",
    "    aspect=1.8,\n",
    ")\n",
    "g.map(plt.plot, \"Year\", \"Mean score\")\n",
    "g.set_titles(\"{col_name}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see three prominent trends here. \n",
    "* Social sciences first decline in popularity until around 1970, after which they start bouncing back, and have been growing to this day. Humanities follow a more mellow development of roughly the same shape.\n",
    "* Contrasting with that, Science follows an almost opposite development, rising and then falling. Especially notable is the big bulge around the inflection point, in the 60s.\n",
    "* Eco/Geo follows a pretty steady pattern until the 80s, after which it has settled into a decline, that may have leveled off as of late.\n",
    "\n",
    "The reversal of the Social/Humanities vs Science trend coincides roughly with the explosion of PhD production from smaller institutions that we observed earlier, but I'm unsure as to how that connection should be interpreted, and whether these events truly are connected. Regardless, the V-shaped competing curves of soft vs hard fields is clearly the grand narrative in these plots. It would be very interesting to try to compare it to other societal and academic trends, such as gender distributions of PhD graduates, changes in academic funding in the UK, and data on students at earlier stages in their education, such as the popularity of different undergraduate majors and changes in primary and secondary education curricula. For the moment, though, this remains work to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can observe academic trends for individual institutions. For instance, here are the same plots as above, but specific to some of the golden triangle universities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc7b7ec297f34b0eb6c7847124633d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_insts = [\n",
    "    \"University of Cambridge\",\n",
    "    \"Imperial College London\",\n",
    "    \"University of Oxford\",\n",
    "    \"London School of Economics and Political Science (University of London)\",\n",
    "]\n",
    "meanscores_by_yearandinst = (\n",
    "    df[df.Institution.apply(lambda x: x in select_insts)]\n",
    "    .groupby([\"Institution\", \"Year\"])\n",
    "    .mean()  # Average scores for each year and institution\n",
    "    .unstack(\"Institution\")\n",
    "    .fillna(0.0)\n",
    "    # Rolling Gaussian average with std=3.0.\n",
    "    .rolling(10, center=True, win_type=\"gaussian\")\n",
    "    .mean(std=3.0)\n",
    "    .dropna()\n",
    "    # Prepare to a format that sns.FacetGrid likes.\n",
    "    .stack(level=[0, 1])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_1\": \"Field\", 0: \"Mean score\"})\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    meanscores_by_yearandinst,\n",
    "    col=\"Field\",\n",
    "    hue=\"Institution\",\n",
    "    # We pick a specific order for the institutions so that with the default\n",
    "    # color scheme the colors roughly match the official colors of the\n",
    "    # universities (Oxford is blue, Cambridge is red).\n",
    "    hue_order=[\n",
    "        \"University of Oxford\",\n",
    "        \"Imperial College London\",\n",
    "        \"London School of Economics and Political Science (University of London)\",\n",
    "        \"University of Cambridge\",\n",
    "    ],\n",
    "    col_wrap=2,\n",
    "    sharex=True,\n",
    "    sharey=False,\n",
    ")\n",
    "g.map(plt.plot, \"Year\", \"Mean score\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.add_legend()\n",
    "# Seaborn puts the legend in a silly place, so move it manually.\n",
    "g._legend.set_bbox_to_anchor((0.8, 0.25));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've chosen to smooth the curves out a bit by doing a Gaussian rolling average of the data over time. Since there aren't that many theses coming out from a single institution in a single year the plots would look quite noisy and hard to read otherwise.\n",
    "\n",
    "Hardly surprisingly, London School of Economics is heavily leaning towards humanities and social sciences, and consistently puts out PhD theses that score higher in those fields than other universities. Less obviously, the above plot also provides evidence for the often-heard view that of the Oxbridge two, Cambridge is the more science heavy one, whereas Oxford leans towards humanities and social sciences. Imperial lives up to its reputation of being a science school first and foremost. Less expected is its huge growth in Biomed in the 90s and early 2000s, but [Wikipedia](https://en.wikipedia.org/wiki/Imperial_College_London#20th_century) informs me that this is probably due to mergers with several medical institutions around that time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epilogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've certainly gotten my mileage from this thesis metadata set. What was supposed to be a quick exercise in learning basic pandas turned into an exploration far beyond my original intent. One could keep exploring trends for individual words and institutions indefinitely, but for now I'm satisfied with the examples provided above. Beyond that however, there are a few additional ideas that I have considered, but haven't gotten around to doing:\n",
    "* Look for thesis titles with abnormal word combinations. Perhaps take the average graph distance of the words in the title, and rank titles based on that. Outlier theses that respect no field boundaries? Yes please!\n",
    "* Visualize the co-occurrence graph. The main problem here is that the graph is relatively densely connected. It has ~20,000 words in it, and thus some 400 million possible edges, out which, if I recall correctly, around 6% exist with a non-zero weight. I don't know much about graph visualization, but this sounds to me like a challenge. Visually seeing the words corresponding to different fields cluster would be quite satisfying though.\n",
    "* Combine the data here with some university rankings, and see if we can predict rankings based on thesis metadata. Credit for this idea to TODO.\n",
    "* Find other data sets that we could combine our analysis with. Especially interesting could be data on academic funding.\n",
    "* Test out some hypotheses for word popularity. For instance, at one point I was considering the hypothesis that humanities and social sciences words have less peaked popularity profiles over time compared to science words. \n",
    "* Dig deeper into community finding algorithms for networks, and see if we can get something other than the Louvain algorithm to work. I already briefly tried some stochastic block models and label propagation, but, not being an expert, my lack of success with them may be entirely due to user errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - We vs I\n",
    "# - Credit feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test any of the above ideas, or otherwise build further on what I've done here, you more than welcome to: It's all MIT licensed (read: do whatever you want with it). I would also be interested in hearing about it if you do so: markus@mhauru.org"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
