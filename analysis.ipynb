{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring trends in UK academia using PhD thesis metadata\n",
    "#### Markus Hauru, January 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 2009 the British Library has been operating its [Electronic Theses Online Service](https://ethos.bl.uk) or EThOS service, that keeps track of PhD theses accepted at UK higher education institutions. The Library also makes public a [dataset](https://data.bl.uk/ethos/) of metadata for all theses in their catalogue. Quoting the EThOS website,\n",
    "\n",
    ">The EThOS dataset lists virtually all UK doctoral theses ever awarded, some 500,000 dating back to 1787. All UK HE institutions are included, but we estimate records are missing for around 10,000 titles (2%).\n",
    "\n",
    "The latest version of the dataset is from March 2018. The data includes title, year, author, and institution for each thesis, as well as a link to a full record of each thesis, which may or may not include things like keywords or access to full texts, depending on the thesis.\n",
    "\n",
    "In this notebook I explore this dataset to identify historical trends in UK academia or other interesting features it may display. While an interesting project in itself, this notebook is also an exercise for myself, to learn some basic exploratory data science tools and techniques, like basics of pandas, some network analysis, and new visualization te."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, some imports of python libraries we'll be needing, and loading the data file into a pandas DataFrame. The script will automatically download the data file into the current working directory if it isn't there yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator as opr\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import community  # Network community finding\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm  # color maps\n",
    "import seaborn as sns\n",
    "# for pretty-printing in notebooks\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pretty and interactive matplotlib plots in Jupyter Lab.\n",
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = \"EThOSCSV_201803.csv\"\n",
    "if not os.path.isfile(datafile):\n",
    "    # Download and/or unzip the data file from the EThOS website.\n",
    "    # We need a couple more imports for this.\n",
    "    import zipfile\n",
    "\n",
    "    datazip = \"EThOSCSV_201803.zip\"\n",
    "    if not os.path.isfile(datazip):\n",
    "        import requests\n",
    "\n",
    "        dataurl = \"https://data.bl.uk/ethos/EThOSCSV201803.zip\"\n",
    "        with requests.Session() as s:\n",
    "            headers = {\n",
    "                \"User-agent\": \"Mozilla/5.0 (X11; Linux i686; rv:64.0) Gecko/20100101 Firefox/64.0\"\n",
    "            }\n",
    "            r = s.get(dataurl, headers=headers)\n",
    "            with open(datazip, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "    with zipfile.ZipFile(datazip, \"r\") as z:\n",
    "        z.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(datafile, encoding=\"ISO-8859-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what the rows in the DataFrame look like, here are some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Institution</th>\n",
       "      <th>Date</th>\n",
       "      <th>Qualification</th>\n",
       "      <th>EThOS URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computation and measurement of turbulent flow ...</td>\n",
       "      <td>Loizou, Panos A.</td>\n",
       "      <td>University of Manchester</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prolactin and growth hormone secretion in norm...</td>\n",
       "      <td>Prescott, R. W. G.</td>\n",
       "      <td>University of Newcastle upon Tyne</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Influence of strain fields on flame propagation</td>\n",
       "      <td>Mendes-Lopes, J. M. C.</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Connectivity, flow and transport in network mo...</td>\n",
       "      <td>Robinson, Peter Clive</td>\n",
       "      <td>University of Oxford</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The theory and implementation of a high qualit...</td>\n",
       "      <td>Lower, K. N.</td>\n",
       "      <td>University of Bristol</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title                  Author  \\\n",
       "0  Computation and measurement of turbulent flow ...        Loizou, Panos A.   \n",
       "1  Prolactin and growth hormone secretion in norm...      Prescott, R. W. G.   \n",
       "2    Influence of strain fields on flame propagation  Mendes-Lopes, J. M. C.   \n",
       "3  Connectivity, flow and transport in network mo...   Robinson, Peter Clive   \n",
       "4  The theory and implementation of a high qualit...            Lower, K. N.   \n",
       "\n",
       "                         Institution    Date   Qualification  \\\n",
       "0           University of Manchester  1989.0  Thesis (Ph.D.)   \n",
       "1  University of Newcastle upon Tyne  1983.0  Thesis (Ph.D.)   \n",
       "2            University of Cambridge  1983.0  Thesis (Ph.D.)   \n",
       "3               University of Oxford  1984.0  Thesis (Ph.D.)   \n",
       "4              University of Bristol  1985.0  Thesis (Ph.D.)   \n",
       "\n",
       "                                           EThOS URL  \n",
       "0  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  \n",
       "1  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  \n",
       "2  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  \n",
       "3  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  \n",
       "4  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first observation to make is that the data is remarkably clean. There are a few NaNs that we need to drop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaNs: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows with NaNs: {}\".format(df.isnull().any(axis=1).sum()))\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but with that done, there's little else to do. The Qualification column holds some oddities and misunderstandings, and there are few suspiciously short titles (my favourite being the 1977 thesis called \"Beds\"), but other than that, things seem in order. Most notably, the Institution and Date fields, which we'll be getting a lot of mileage from, seem spotless, without even a single typoed university name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is of some concern are the 10,000 or so theses that the British Library estimates are missing from the data set. Nowadays that's about a year's worth of theses, but it's also roughly the total number of theses in the data from before 1956. Now clearly some of the missing ones are from the last few years: Even though the set is from 2018, the number of theses in the database drops sharply after 2015, even though it's been climbing up to that point. But there's probably a significant number missing from the early years as well, considering issues of record keeping. To illustrate the late and early parts of the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesiscounts_by_year = df.Date.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16f2dea0ded4767b2438c1794763ecf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(7, 3))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(thesiscounts_by_year[2005:])\n",
    "plt.ylabel(\"Number of PhD theses\")\n",
    "plt.xlabel(\"Year\")\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(thesiscounts_by_year[:1940])\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the end of WW1 the sample sizes for each year are clearly quite low, and based on this, as well as some odd features of the data pre-WW1, I've chosen to limit the time period to study to 1925-2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_cutoff = 2015\n",
    "early_cutoff = 1925\n",
    "thesiscounts_by_year = thesiscounts_by_year[early_cutoff:late_cutoff]\n",
    "df = df[(early_cutoff <= df.Date) & (df.Date <= late_cutoff)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also drop the columns we won't be using for anything. Makes for nicer reading when we print out slices of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"Qualification\", \"EThOS URL\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of theses per year and institution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that basic setup out of the way, let's get into this.\n",
    "\n",
    "First, a basic look at the number of PhD theses accepted in the UK over our chosen time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f1ceeb575904ed9a8bb1e57107766c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.semilogy(thesiscounts_by_year)\n",
    "plt.ylabel(\"Number of PhD theses.\")\n",
    "plt.xlabel(\"Year\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the logarithmic vertical axis. The two word summary here would clearly be \"exponential growth\". In 1925 a bit less than a hundred PhDs were awarded, and now we are approaching 20 000 per year. The most notable features are\n",
    "* a huge dip during WW2 in the middle of the otherwise steady exponential period from 1925 to 1960. (In the early-times plot a couple of cells above you can also see a dip during WW1, although the signal there is more noisy.) There's a corresponding bump a few years after WW2, presumably from people who postponed the start of their studies until after the war and are then graduating in large batches.\n",
    "* a period of even faster exponential growth in the 1960-1980 window. One explanation for this is baby boomers hitting typical PhD age (compare the above to a UK birth rate plot [here](https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/bulletins/birthsummarytablesenglandandwales/2017) and shift by 25 or so years), but probably the most important driver of this growth is the huge structural change in academia that we'll see in a moment.\n",
    "* a more moderate but steady pace from roughly 1980 onwards, that may be slowing down lately.\n",
    "\n",
    "Note that when I talk about rate or pace of growth here I'm talking in multiplicative terms, meaning for instance that since 1980 the number of new PhDs has been growing by a roughly constant _percentage_ per year, not a constant number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look at the contributions of different institutions to the total output of PhDs. The below plot shows the total thesis count per year divided between different universities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesiscounts_by_yearandinst = (\n",
    "    df.groupby([\"Institution\", \"Date\"])\n",
    "    .size()\n",
    "    .unstack(\"Institution\")\n",
    "    .fillna(0.0)\n",
    ")\n",
    "# Sort institutions based on PhD output in the last year of the time window.\n",
    "thesiscounts_by_yearandinst = thesiscounts_by_yearandinst.sort_values(\n",
    "    late_cutoff, axis=\"columns\", ascending=False\n",
    ")\n",
    "instratios_by_year = thesiscounts_by_yearandinst.divide(\n",
    "    thesiscounts_by_year, axis=\"index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85232829b0e5401197bca46f24fe4f3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can't show the full legend with dozens of entries, so we restrict to a few of the\n",
    "# most important ones.\n",
    "num_legendentries = 10\n",
    "\n",
    "# We use one of matplotlibs color maps with 20 colors since we have a lot areas to plot,\n",
    "# but to not have similar colors right next to each other we mix up the order.\n",
    "colors = cm.tab20b.colors\n",
    "num_colors = len(colors)\n",
    "colors = [colors[i * 3 % num_colors] for i in range(num_colors)]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "lines = plt.stackplot(\n",
    "    instratios_by_year.index, instratios_by_year.values.T, colors=colors\n",
    ")\n",
    "plt.ylabel(\"Ratio of PhDs by institution\\n to total number of PhDs that year\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.ylim(0.0, 1.0)\n",
    "plt.xlim(early_cutoff, late_cutoff)\n",
    "# Some fancy formatting of the legend. We show the most important legend entries in the same order\n",
    "# that they appear in in the figure and position the legend out of the way.\n",
    "plt.legend(\n",
    "    reversed(lines[:num_legendentries]),\n",
    "    reversed(instratios_by_year.columns[:num_legendentries]),\n",
    "    loc=\"lower left\",\n",
    "    bbox_to_anchor=(1.0, 0.0),\n",
    ")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The legend names a few of the most important instutions, starting from the bottom of the plot.\n",
    "\n",
    "This shows the drastic change in the structure of higher education between roughly 1960 and 1980, that I mentioned above: There's an explosion in the number of PhDs coming from small universities. Until 1960 or so Oxford, Cambridge and Edinburgh produced 70-80% of PhDs in the country. By 1980 it was around to 20%, and has stayed roughly constant since. They remain the largest PhD factories, together with Manchester and Imperial College London, but the field has been filled with dozens of smaller institutions. During the shift many new universities were founded (Sussex 1961, York 1963, Warwick 1965, etc.), many others expanded greatly, and various colleges and other educational institutions were turned into universities. I'm not a historian, but for some background and context, check Wikipedia on for instance [University Grant Comission](https://en.wikipedia.org/wiki/University_Grants_Committee_(United_Kingdom)), [Robbins Report](https://en.wikipedia.org/wiki/Robbins_Report), and [Education Act of 1962](https://en.wikipedia.org/wiki/Education_Act_1962). I was previously aware that there was an expansion of higher education in many Western countries around this time, but I have to say that the magnitude and speed of the shift in the above plot surprised me.\n",
    "\n",
    "There are also some interesting individual stories in this same plot. Oxford used to dominate its rival Cambridge with a doctoral output several times larger until the late 60s, after which the two have been roughly equal. Even Oxford was for a time ecplised by Edinburgh though, which in some years produced more than 40% of all PhDs in the UK. (It should be said though that I suspect that these early years may suffer from a significant number of missing theses, that may skew the data to some degree.) Imperial has an interesting bulge of activity during the 1960s as it went through [a rapid expansion](https://en.wikipedia.org/wiki/Imperial_College_London#20th_century), a bit before many others followed suit. Overall, this millenium there's been again a slight squeeze, where the big institutions have been growing and the smaller ones have been shrinking, although these changes are very minor compared to the opposite explosion in the 60s and 70s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to underline the fact that the fall in the relative importance of the top few institutions is not at all due to shrinking on their part but merely the growth of others, here are the absolute numbers of PhDs produced by the 8 institutions that contributed the most in 1960."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14497d0c8e9b4412ab1040f01f080cb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_insts = 8\n",
    "counts_top = thesiscounts_by_yearandinst.sort_values(\n",
    "    1960, axis=1, ascending=False\n",
    ")\n",
    "plt.figure(figsize=(10, 5))\n",
    "lines = plt.plot(counts_top.iloc[:, :num_insts])\n",
    "plt.ylabel(\"Number of PhDs produced\")\n",
    "plt.xlabel(\"Year\")\n",
    "plt.legend(\n",
    "    lines,\n",
    "    counts_top.columns[:num_insts],\n",
    "    loc=\"lower left\",\n",
    "    bbox_to_anchor=(1.0, 0.0),\n",
    ")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words, words, words..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've only looked at where and when PhD theses have been written. The dataset doesn't hold that much more information to toy with. It doesn't list departments or faculties, keywords or research fields, nor do I have easy access to full texts of the theses. But what it does have is the titles of these theses. Let's see what we can learn from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the title analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to looking at trends in titles, we need to spend a moment setting up some machinery that we can then milk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now thesis titles aren't exactly prose. They typically aren't full sentences and their grammatical forms vary wildly, which makes analysis of their linguistic structure tricky and, I would guess, in many cases a bit futile. Unlike with more structured text, almost as much information as in the title itself is contained in just a list of words appearing in the title. Lists of words are also easy to analyse, so we'll go with that.\n",
    "\n",
    "To start, we strip the titles of any punctation and so called stop words like articles and prepositions that don't tell us much, and make everything lower case. We could also do what's called stemming, and collapse for instance \"study\", \"studies\", \"studied\" and \"studying\" all into a single word. I choose not to do this because as we'll see below, different inflections of the same stem word sometimes appear in significantly different kinds of titles. Moreover, with academic vocabulary, doing this properly isn't straight-forward: the stemming algorithm should for instance know to combine \"phenomenon\" and \"phenomena\", but perhaps not \"phenomenal\" or \"phenomenology\", and certainly not \"phenolic\". A quick experiment that I did with one stemmer also suggested that the following results wouldn't be greatly affected.\n",
    "\n",
    "Another choice I make is to remove hyphens as unnecessary punctuation. Here you lose some and you win some. \"Post-modern\" should certainly be grouped together with \"postmodern\", but \"biodiversity-ecosystem\" (which appears in 5 titles) would be better split into two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitstr(s):\n",
    "    \"\"\" For a string, remove most punctuation, lower case, and split into words. Return the words.\n",
    "    \"\"\"\n",
    "    puncts = \"!\\\"&'(),./:;<=>?[\\\\]`{|}-\"\n",
    "    return s.lower().translate(str.maketrans(\"\", \"\", puncts)).split()\n",
    "\n",
    "\n",
    "def filter_stopwords(l):\n",
    "    \"\"\" Take a list of strings, filter out prepositions, articles, and other stop words.\n",
    "    We use a list of English stop words found here: https://www.textfixer.com/tutorials/common-english-words.txt\n",
    "    \"\"\"\n",
    "    # We stop black from automatically splitting this across a gazillion lines with the fmt: off/on.\n",
    "    # fmt: off\n",
    "    stops = [\"a\", \"able\", \"about\", \"across\", \"after\", \"all\", \"almost\", \"also\", \"am\", \"among\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"dear\", \"did\", \"do\", \"does\", \"either\", \"else\", \"ever\", \"every\", \"for\", \"from\", \"get\", \"got\", \"had\", \"has\", \"have\", \"he\", \"her\", \"hers\", \"him\", \"his\", \"how\", \"however\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"just\", \"least\", \"let\", \"like\", \"likely\", \"may\", \"me\", \"might\", \"most\", \"must\", \"my\", \"neither\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"often\", \"on\", \"only\", \"or\", \"other\", \"our\", \"own\", \"rather\", \"said\", \"say\", \"says\", \"she\", \"should\", \"since\", \"so\", \"some\", \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \"these\", \"they\", \"this\", \"tis\", \"to\", \"too\", \"twas\", \"us\", \"wants\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"would\", \"yet\", \"you\", \"your\"]\n",
    "    # fmt: on\n",
    "    return tuple(s for s in l if s not in stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Words\"] = df[\"Title\"].apply(splitstr).apply(filter_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A handy thing to have: A function that, given a word, spits out example titles where it appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_titles(word, n=1):\n",
    "    titles_with_word = df[df[\"Words\"].apply(lambda x: word in x)][\"Title\"]\n",
    "    n = min(n, len(titles_with_word))\n",
    "    examples = tuple(titles_with_word.sample(n).values)\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the title analysis going, we want three different objects:\n",
    "* A DataFrame that lists the total number of titles in which each word appears.\n",
    "* The same thing but now per year. (We could do per institution as well, but let's leave that for later.)\n",
    "* A so called co-occurrence graph (or network), i.e. a weighted graph where nodes are words and edges tell us which words appear together in titles and how often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_by_year = (\n",
    "    df.groupby(\"Date\")[\"Words\"]\n",
    "    .apply(lambda x: pd.Series(np.concatenate(x.tolist())).value_counts())\n",
    "    .unstack(\"Date\")\n",
    "    .fillna(0.0)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_wordcounts = wordcounts_by_year.sum(axis=0)\n",
    "# Sort both total_wordcounts and wordcounts_by_year to have the most common words first.\n",
    "order = (-total_wordcounts).argsort()\n",
    "total_wordcounts = total_wordcounts[order]\n",
    "allwords = total_wordcounts.index\n",
    "wordcounts_by_year = wordcounts_by_year.reindex(allwords, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the occurrences of each word per a 1000 thesis, in each given year, to measure the relative popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordratios_by_year = 1000*wordcounts_by_year.divide(thesiscounts_by_year, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the co-occurrence graph requires some thought. First off, we've got around 190,000 distinct words in about 450,000 titles (academics like their jargon). However, only 20,000 or so of them appear in more than 10 thesis titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 117488. Number of theses: 200000. Number of words with more than 10 occurrences: 13123.\n"
     ]
    }
   ],
   "source": [
    "wordcount_cutoff = 10\n",
    "print(\n",
    "    \"Number of unique words: {}. Number of theses: {}. Number of words with more than {} occurrences: {}.\".format(\n",
    "        len(total_wordcounts),\n",
    "        len(df),\n",
    "        wordcount_cutoff,\n",
    "        (total_wordcounts > wordcount_cutoff).sum(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus I choose to restrict the graph to this subset of words, since it makes it computationally much lighter to handle, without causing much of a loss: Words that only occur in a handful of thesis titles probably wouldn't add much to our analysis anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainwords = allwords[total_wordcounts > wordcount_cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that when making word co-occurrence graphs of texts, co-occurrences are often given more weight if the words are next to each rather than just in the same sentence. I don't make this distinction since the titles are pretty short anyway, and word orders in titles can be unusual, making proximity less relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the graph/network, I use the [NetworkX](https://networkx.github.io/) package. It represents graphs as adjacency lists, which suits our quite sparse co-occurrence graph well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_graph = nx.Graph()\n",
    "cooc_graph.add_nodes_from(mainwords)\n",
    "for words in df[\"Words\"]:\n",
    "    words = tuple(filter(lambda w: w in mainwords, words))\n",
    "    for i in range(len(words)):\n",
    "        wi = words[i]\n",
    "        # The second loop starts from i+1 to avoid double-counting.\n",
    "        for j in range(i + 1, len(words)):\n",
    "            wj = words[j]\n",
    "            if wi in cooc_graph[wj]:\n",
    "                # This edge already exists, increment the weight\n",
    "                cooc_graph[wi][wj][\"weight\"] += 1.0\n",
    "            else:\n",
    "                # This edge doesn't yet exist, create it.\n",
    "                cooc_graph.add_edge(wi, wj, weight=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have it now, the edge weights of the graph just count the number of theses in which both words appear. This isn't quite what we want, since it's dominated by commonly occuring words. There's several different ways we could normalize the weights. Dividing by the total appearance counts of the words (diagonal of the adjacency matrix) would make the weight of the edge between nodes A and B represent a conditional probability (or rather a frequency) P(A|B) of word A appearing in a title given that word B appears. We could also make a bidirectional graph that has both P(A|B) and P(B|A) stored for each edge. Or we could do a number of more symmetric normalizations that don't have as concrete an interpretation. A choice that I've found works well for what I want to do with this graph later is normalizing the edge between A and B by average of the frequencies of A and B. This is symmetric (the graph is undirected) and roughly speaking gives a large weight for edges that connect two words that appear roughly equally frequently and often together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w1, w2 in cooc_graph.edges:\n",
    "    avg_count = (total_wordcounts[w1] + total_wordcounts[w2]) / 2.0\n",
    "    cooc_graph[w1][w2][\"weight\"] /= avg_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another handy function to have: One that, given a word, gets related words from the co-occurrence graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_words(w, n=5):\n",
    "    \"\"\" For a given word w, get the n (by default n=5) words with\n",
    "    heaviest edges connected to w in the co-occurrence graph.\n",
    "    Returns a list of tuples (neighbour word, weight), sorted by weight.\n",
    "    \"\"\"\n",
    "    if w not in cooc_graph:\n",
    "        # This word is not in the co-occurrence network.\n",
    "        return ()\n",
    "    node = cooc_graph[w]\n",
    "    neighbourlist = ((k, v[\"weight\"]) for k, v in node.items())\n",
    "    return sorted(neighbourlist, key=opr.itemgetter(1), reverse=True)[:n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, lets wrap in a function something we'll be doing repeatedly: Plotting the popularity of a word over time and printting out some often co-occuring words and example titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_word_summary(w, n_related=3, n_examples=3, plot=True):\n",
    "    print(\"Word: {}\".format(w))\n",
    "    if n_related > 0:\n",
    "        print(\"Related words:\")\n",
    "        [\n",
    "            print(\" {}: {}\".format(*w))\n",
    "            for w in get_related_words(w, n=n_related)\n",
    "        ]\n",
    "    if n_examples:\n",
    "        print(\"Example titles:\")\n",
    "        [\n",
    "            print(\" {}\".format(title))\n",
    "            for title in get_example_titles(w, n=n_examples)\n",
    "        ]\n",
    "    if plot:\n",
    "        fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "        ax1.plot(wordcounts_by_year[w], color=\"black\")\n",
    "        title = \"Number and proportion of theses\\nwith '{}' in the title\".format(\n",
    "            w\n",
    "        )\n",
    "        ax1.set_title(title)\n",
    "        ax1.set_ylabel(\"Absolute umber (black)\".format(w))\n",
    "        ax1.set_xlabel(\"Year\")\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(wordratios_by_year[w], color=\"darkred\")\n",
    "        ax2.set_ylabel(\"Per 1000 theses (red)\".format(w))\n",
    "        fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends in individual words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all that set-up out of the way, let's see if these titles hold some interesting. First off, what are the most commonly occuring words in thesis titles, and how does their popularity vary over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: study\n",
      "Related words:\n",
      " case: 0.20922909880564602\n",
      " comparative: 0.11863266678896839\n",
      " development: 0.06386439517164626\n",
      "Example titles:\n",
      " Ecological dynamics and human welfare : a case study of population, health and nutrition in Zimbabwe\n",
      " Theoretical study of binding energy and electronic energy levels in small clusters of metal atoms\n",
      " A case study of ESP for medical workplaces in Saudi Arabia from a needs analysis perspective\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1ed3baa96524d32b16b5f88d7432d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: studies\n",
      "Related words:\n",
      " structural: 0.0948864975257246\n",
      " synthesis: 0.05163344407530454\n",
      " synthetic: 0.05063073886552819\n",
      "Example titles:\n",
      " Magneto-optical studies in semiconductors\n",
      " Permeation studies of hydrogen in nickel, molybdenum and M316 stainless steel : the influence of phase boundary processes\n",
      " Industrial boiler system corrosion inhibitors : Studies on the high temperature reactions and properties of aliphatic amines in water\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "823fca56f70e4aff8671f528642980b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: development\n",
      "Related words:\n",
      " system: 0.06889233531956575\n",
      " application: 0.06532399299474606\n",
      " study: 0.06386439517164626\n",
      "Example titles:\n",
      " An exploratory study of research and development in construction in the developing countries of the middle East\n",
      " Development of CCD and EM-CCD technology for high resolution X-ray spectrometry\n",
      " The development of a methodology for automated sorting in the minerals industry\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e8ea008487d495d83c01497504388a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_topwords = 3\n",
    "for w in allwords[:num_topwords]:\n",
    "    output_word_summary(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing too exciting here: Generic terms that appear in all kinds of titles. Both \"study\" and \"studies\" have clearly fallen into relative disuse since the 70s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more interesting than the most popular words, are those whose popularity has been fleeting. One could get at this in a million different ways, but here's one that I found interesting. For each decade, find the top 3 words whose popularity that decade was the most disproportionately large compared to their popularity in the whole time window. In other words, rank words by number of occurences in a given decade, divided by total number of occurences. To not have this be dominated by words occuring in only a handful of theses, further restrict to words with at least 100 occurences in total. I'll also print for each word some of the commonly co-occurring words, to give some context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1930s:\n",
      "cases            ['review', 'hundred', 'pathological']\n",
      "substances       ['humic', 'similar', 'toxic']\n",
      "tuberculosis     ['mycobacterium', 'pulmonary', 'bovine']\n",
      "-------------------------------------------------------------------------------\n",
      "1940s:\n",
      "substances       ['humic', 'similar', 'toxic']\n",
      "tuberculosis     ['mycobacterium', 'pulmonary', 'bovine']\n",
      "constitution     ['discursive', 'alloys', 'certain']\n",
      "-------------------------------------------------------------------------------\n",
      "1950s:\n",
      "substances       ['humic', 'similar', 'toxic']\n",
      "microorganisms   ['methionine', 'photosynthetic', 'swimming']\n",
      "helium           ['superfluid', 'temperatures', 'adsorbed']\n",
      "-------------------------------------------------------------------------------\n",
      "1960s:\n",
      "spectra          ['raman', 'absorption', 'molecules']\n",
      "geology          ['geochemistry', 'area', 'petrology']\n",
      "testament        ['old', 'hebrews', 'epistle']\n",
      "-------------------------------------------------------------------------------\n",
      "1970s:\n",
      "helium           ['superfluid', 'temperatures', 'adsorbed']\n",
      "geology          ['geochemistry', 'area', 'petrology']\n",
      "microscope       ['scanning', 'electron', 'tunnelling']\n",
      "-------------------------------------------------------------------------------\n",
      "1980s:\n",
      "monoclonal       ['antibodies', 'antibody', 'against']\n",
      "sedimentology    ['stratigraphy', 'diagenesis', 'basin']\n",
      "aided            ['computer', 'manufacture', 'design']\n",
      "-------------------------------------------------------------------------------\n",
      "1990s:\n",
      "cloning          ['genes', 'encoding', 'sequencing']\n",
      "parallel         ['algorithms', 'computation', 'processing']\n",
      "expert           ['lay', 'system', 'novice']\n",
      "-------------------------------------------------------------------------------\n",
      "2000s:\n",
      "globalisation    ['firmlevel', 'hegemonic', 'beijing']\n",
      "nanotubes        ['singlewalled', 'carbon', 'walled']\n",
      "internet         ['banking', 'online', 'internet']\n",
      "-------------------------------------------------------------------------------\n",
      "2010s:\n",
      "biomarkers       ['proteomic', 'colorectal', 'stratification']\n",
      "resilience       ['disaster', 'looked', 'vulnerable']\n",
      "interpretative   ['phenomenological', 'experiences', 'clients']\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_topwords = 3\n",
    "for start_year in range(1930, 2020, 10):\n",
    "    end_year = start_year + 10\n",
    "    print(\"{}s:\".format(start_year))\n",
    "    decade_top_words = (\n",
    "        wordcounts_by_year[start_year:end_year].sum() / total_wordcounts\n",
    "    )[total_wordcounts > 100].sort_values(ascending=False)\n",
    "    for w in decade_top_words.index[:num_topwords]:\n",
    "        # The w[0] picks just the word, and leaves out the weight.\n",
    "        neighbours = [w[0] for w in get_related_words(w, n=3)]\n",
    "        print(\"{:15}  {}\".format(w, neighbours))\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 30s, 40s, and 50s look quite similar, with pretty generic terminology, especially science terminology trending high, but from that point on things get interesting. Plenty of particle physics in the 60s and 70s (a bubble chamber is a type of particle physics experiment and GeV/c, gigaelectronvolts per speed of light, is a a unit for momentum in particle physics), when new particles were constantle being discovered. The 80s, 90s and 2000s have a lot of computer science and communication technology words in the lead (ATM in most cases stands for asynchronous transfer mode), giving way to \"mindfulness\" and \"resilience\" in the last decade. They also highlight the relative lack of humanities and social sciences terminology in the list, but we'll come back that later.\n",
    "\n",
    "Just for highlighting, here's a bit more detail on a few select words from the above list. The prospective rabbit holes are endless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: monoclonal\n",
      "Related words:\n",
      " antibodies: 0.5106382978723404\n",
      " antibody: 0.19148936170212766\n",
      " against: 0.04238921001926782\n",
      "Example titles:\n",
      " The biology of immunoglobulin free light chains in kidney disease : a study of Monoclonal and Polyclonal light chains\n",
      " Physical and chemical studies of monoclonal antibodies to lysozyme\n",
      " Studies on pig kidney microvillar membrane proteins using monoclonal antibodies\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb7da0a2f544d9388869920628b6e60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: globalisation\n",
      "Related words:\n",
      " firmlevel: 0.03508771929824561\n",
      " hegemonic: 0.034782608695652174\n",
      " beijing: 0.032520325203252036\n",
      "Example titles:\n",
      " Globalisation and the negotiation of identity in South Asian diasporic fiction in Britain\n",
      " Globalisation and ethnic integration in corporate Malaysia : the case of public-listed companies in the Klang Valley and Penang\n",
      " Globalisation and the labour market : an analysis of job stability and job security in Britain\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c37382c0664059a682b7e288a1c46a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word: graphene\n",
      "Related words:\n",
      " epitaxial: 0.10909090909090909\n",
      " cvd: 0.06382978723404255\n",
      " dots: 0.06015037593984962\n",
      "Example titles:\n",
      " Acoustoelectric transport in graphene\n",
      " Raman spectroscopy of graphene, its derivatives and graphene-based heterostructures\n",
      " The optical characterisation of graphene\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af853e511784c94940bc9fba33e6794",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_words = [\"monoclonal\", \"globalisation\", \"graphene\"]\n",
    "for w in select_words:\n",
    "    output_word_summary(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12 points go to Biomed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual words like those above tell us stories of individual scientific discoveries and hot topics, but they have their limits if we want to study larger academic trends. However, one would naturally expect the co-occurrences of words in titles to follow patterns, where words related to fields and subfields would often appear together. Our next goal is to see if this indeed happens, and assuming it does (yes, it does), use it to\n",
    "* identify what are the academic field distinctions that the co-occurence graph holds.\n",
    "* analyse how the popularities of these different fields have varied.\n",
    "\n",
    "So we want to find groups of words that typically appear together. This could be called clustering, but in the context of networks/graphs like our co-occurrence graph, it usually goes by the name of community structure. There's plenty of research done on developing algorithms for identifying communities. We use below one quite well-known one, the [Louvain algorithm](https://arxiv.org/abs/0803.0476), which is a heuristic algorithm based on optimizing the modularity of the graph, i.e. minimizing the weight of edges connecting different communities and maximizing the weight of edges internal to communities. I tried a few other methods as well, most notably label-propagation and stochastic block models, but at least with the parameters that I tried they seemed to produce communities that were either very small or did not match well my human intuition of which words I would expect to be related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this notebook yourself, you can safely go make a cup of tea at this point. The community finding takes around  minutes on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community.best_partition(cooc_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# louvain.best_partition returns a dictionary of {word: community_label}.\n",
    "# Lets turn that into a DataFrame and extract the lists of words belonging to each community.\n",
    "community_labels = pd.DataFrame(\n",
    "    {\"Label\": tuple(partition.values())},\n",
    "    index=partition.keys(),\n",
    "    dtype=\"category\",\n",
    ")\n",
    "communities = tuple(\n",
    "    map(tuple, community_labels.groupby(\"Label\").groups.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of communities: 8\n",
      "Numbers of words in the communities: [3357, 3621, 2760, 1818, 1558, 2, 3, 4]\n",
      "Some example words from each community:\n",
      "('study', 'development', 'analysis', 'case', 'social', 'between', 'reference', 'use', 'theory', 'management', 'approach', 'new', 'education', 'performance', 'aspects', 'learning', 'evaluation', 'impact', 'health', 'english', 'policy', 'british', 'towards', 'special', 'change', 'influence', 'children', 'practice', 'uk', 'england', 'assessment', 'process', 'political', 'comparative', 'language', 'relationship', 'through', 'information', 'critical', 'politics')\n",
      "\n",
      "('studies', 'investigation', 'using', 'systems', 'control', 'modelling', 'design', 'system', 'synthesis', 'behaviour', 'properties', 'structure', 'model', 'application', 'applications', 'high', 'models', 'based', 'methods', 'dynamics', 'structural', 'techniques', 'processes', 'flow', 'data', 'networks', 'metal', 'experimental', 'energy', 'power', 'problems', 'reactions', 'structures', 'optical', 'processing', 'water', 'related', 'materials', 'surface', 'compounds')\n",
      "\n",
      "('role', 'effects', 'human', 'characterisation', 'novel', 'effect', 'molecular', 'cell', 'factors', 'cells', 'growth', 'disease', 'during', 'regulation', 'function', 'interactions', 'production', 'protein', 'mechanisms', 'cancer', 'genetic', 'response', 'gene', 'interaction', 'expression', 'functional', 'potential', 'treatment', 'clinical', 'activity', 'metabolism', 'responses', 'stress', 'proteins', 'patients', 'identification', 'rat', 'resistance', 'dna', 'investigating')\n",
      "\n",
      "('early', 'century', 'art', 'late', 'music', 'modern', 'literature', 'poetry', 'selected', 'fiction', 'de', 'interpretation', 'works', 'church', 'french', 'religious', 'literary', 'thought', 'john', 'narrative', 'theology', 'significance', 'form', 'roman', 'christian', 'philosophy', 'tradition', 'translation', 'medieval', 'old', 'origins', 'novels', 'nineteenth', 'text', 'religion', 'theatre', 'edition', 'popular', 'ad', 'ethics')\n",
      "\n",
      "('evolution', 'l', 'ecology', 'distribution', 'scotland', 'species', 'population', 'north', 'central', 'spatial', 'age', 'west', 'soil', 'southern', 'land', 'marine', 'western', 'region', 'sea', 'area', 'climate', 'conservation', 'diversity', 'eastern', 'soils', 'populations', 'ecological', 'sensing', 'temporal', 'landscape', 'lower', 'river', 'upper', 'impacts', 'variability', 'forest', 'basin', 'microbial', 'fish', 'atlantic')\n",
      "\n",
      "('binocular', 'monocular')\n",
      "\n",
      "('buenos', 'aires', 'contention')\n",
      "\n",
      "('americana', 'cockroach', 'periplaneta', 'specifying')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of communities: {}\".format(len(communities)))\n",
    "print(\n",
    "    \"Numbers of words in the communities: {}\".format(\n",
    "        [len(c) for c in communities]\n",
    "    )\n",
    ")\n",
    "print(\"Some example words from each community:\")\n",
    "num_example_words = 40\n",
    "[print(c[:num_example_words], end=\"\\n\\n\") for c in communities];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So according to the Louvain algorithm, our co-occurrence graph has 19 communities, out of 5 are non-tiny. We'll just discard the 14 small ones as uninteresting (not every word needs to be in a community, recall that our graph only includes the ~20,000 most common words anyway), and focus on the 5 big ones.\n",
    "\n",
    "Gladly, they make quite good sense to human intuition. The first one has a lot of social sciences vocabulary in it, we'll call it Social; the second one is clearly hard sciences, maths, and engineering or just Science for short; the second is medicine and biochemistry, aka Biomed; the fourth one clearly Humanities; and the fifth has a theme of ecology and geography, and we'll call it Eco/Geo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point a note about stability of these communities is in order. I've been running the same community finding on graphs made with only subsets of the data set, leaving out some of the theses, and also tried including more or less words in the co-occurrence graph (remember we picked the arbitrary cutoff of only including words with at least 10 occurrences). The main communities fortunately appear quite stable under such perturbations. Including some what less words or less theses sometimes makes Humanities merge with Social sciences, but otherwise the communities stay roughly as they are. Going the other way, including more rare words in the graph may for instance sometimes cause particle physics to separate from the rest of Science. These kinds of granularity differences, of whether a field splits into a further subfields or merges with its academic neighbour, is quite natural, and doesn't in my view undermine the analysis in any significant way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, what we would like to do is assign to each word and thesis title a score or label, that would tell us roughly which field(s) it belongs in. The naive way would be to just use to label each word with its respective community, and count how many humanities words appear in a given title, etc. This doesn't seem quite fair though. Clearly some words are in some sense more \"deeply\" in each community. For instance the word \"theory\" gets grouped into social sciences, but obviously it occurs in other contexts as well, unlike the word \"policy\", which is pretty dead give-away. To account for this effect, we'll give each word scores for how strongly they are connected to each community. This score starts out being 1 for the community the word belongs in and 0 for the others, but we add to it the total weight of edges connecting this word to words in a given community. So a word that is in the humanities community and co-occurs mostly with other humanities words gets a high humanities-score, whereas a word that co-occurs with words from several different communities will have significant scores for all of them. Finally, we'll normalize the scores by the total sum within a field, so for instance the Hum/Soc score of each word will be divided by the sum of Hum/Soc scores of all words. This accounts for the fact that some fields include more words, and these words may be more strongly connected in our co-occurrence graph. We'll also multiply the resulting scores by 10,000, just to produce more human-readable numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, use some key words in each community to anchor the names of the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_keywords = {\n",
    "    \"education\": \"Social\",\n",
    "    \"cell\": \"Biomed\",\n",
    "    \"magnetism\": \"Science\",\n",
    "    \"ecology\": \"Eco/Geo\",\n",
    "    \"philosophy\": \"Humanities\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First give each word a score of 1.0 for the community it belongs in according to the Louvain classification. Drop all the small communities we don't care about, and name the columns of the DataFrame with the names given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordscores = pd.get_dummies(community_labels).astype(np.float_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, field in field_keywords.items():\n",
    "    current_label = wordscores.loc[word, :].idxmax()\n",
    "    wordscores.rename(columns={current_label: field}, inplace=True)\n",
    "for c in wordscores.columns:\n",
    "    if not c in field_keywords.values():\n",
    "        wordscores.drop(c, axis=1, inplace=True)\n",
    "fieldnames = wordscores.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then add to the scores the weights of the edges connecting each word to words of different communities. This is easy to do with a matrix product with the adjacency matrix of the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So to get the scores we want, we need to take the matrix product of the\n",
    "# adjacency matrix of the co-occurrence graph with the current score matrix,\n",
    "# wordscores.values. The graph is way too big to build the whole adjacency\n",
    "# matrix as a dense matrix, but luckily NetworkX has us covered, with the\n",
    "# possibility of creating a scipy sparse matrix instead. However, the pandas\n",
    "# dot function for matrix products insists in converting everything to a dense\n",
    "# matrix to do the product, so we'll have to manually work with the\n",
    "# scipy.sparse matrix instead of a DataFrame. It's not too bad though.\n",
    "adjacency_matrix = nx.to_scipy_sparse_matrix(\n",
    "    cooc_graph, nodelist=wordscores.index, format=\"csr\"\n",
    ")\n",
    "for c in fieldnames:\n",
    "    column_vec = wordscores[c].to_numpy()\n",
    "    scores = adjacency_matrix.dot(column_vec)\n",
    "    scores_series = pd.Series(scores, index=wordscores.index)\n",
    "    wordscores[c] += scores_series\n",
    "del(adjacency_matrix)  # Release the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, normalize the scores, and give all the words we did not include in our graph (the ones with fewer than 10 occurrences) a score of 0.0 for everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordscores *= 10000 / wordscores.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordscores = wordscores.reindex(allwords, fill_value=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the scoring system makes sense, let's check the scores for the top words for each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words with highest scores for Social:\n",
      "               Social   Science    Biomed  Humanities   Eco/Geo\n",
      "education    7.487827  0.528913  0.463062    1.533165  0.535413\n",
      "case         7.363024  0.946124  0.639468    1.121846  1.477142\n",
      "school       7.188815  0.559221  0.580077    1.027660  0.469706\n",
      "students     7.107860  0.530365  0.476748    0.744718  0.353932\n",
      "experiences  6.991886  0.440224  1.060326    0.904114  0.330859\n",
      "teachers     6.859119  0.404379  0.380870    0.745388  0.390275\n",
      "\n",
      "Words with highest scores for Science:\n",
      "                Social   Science    Biomed  Humanities   Eco/Geo\n",
      "using         2.052565  6.305451  2.320687    0.423059  1.365946\n",
      "high          1.360270  6.203214  1.272575    0.292053  0.944587\n",
      "flow          0.668661  5.803994  1.122301    0.141784  1.320929\n",
      "detector      0.228792  5.803482  0.360734    0.212696  0.179432\n",
      "optical       0.442634  5.746755  0.590107    0.136178  0.337319\n",
      "spectroscopy  0.253565  5.665409  0.817261    0.155537  0.416699\n",
      "\n",
      "Words with highest scores for Biomed:\n",
      "              Social   Science    Biomed  Humanities   Eco/Geo\n",
      "cells       0.502633  1.449484  8.580850    0.124940  0.405147\n",
      "cell        0.603537  1.602199  8.430593    0.148021  0.443209\n",
      "receptor    0.335441  0.604717  7.818528    0.100637  0.250623\n",
      "expression  0.627180  0.694089  7.639174    0.444105  0.772320\n",
      "gene        0.484653  0.775863  7.632630    0.158194  0.733364\n",
      "virus       0.430631  0.523520  7.586884    0.212537  0.979824\n",
      "\n",
      "Words with highest scores for Humanities:\n",
      "             Social   Science    Biomed  Humanities   Eco/Geo\n",
      "centuries  1.266447  0.317980  0.189589    9.952712  1.020483\n",
      "century    3.125807  0.391276  0.396624    9.349116  1.059463\n",
      "theology   1.511746  0.247239  0.255275    9.140362  0.287838\n",
      "de         1.182361  0.371778  0.464736    9.083437  0.908162\n",
      "john       1.336736  0.229468  0.201010    9.031066  0.545253\n",
      "la         0.934080  0.261332  0.199003    8.996612  0.658475\n",
      "\n",
      "Words with highest scores for Eco/Geo:\n",
      "            Social   Science    Biomed  Humanities    Eco/Geo\n",
      "l         0.742450  0.729420  2.715215    0.357327  11.241897\n",
      "basin     1.040929  0.578909  0.245447    0.538414  10.171545\n",
      "salmo     0.230521  0.350898  1.099288    0.394535   9.735661\n",
      "atlantic  0.645343  0.695481  0.856248    0.650419   9.636563\n",
      "rocks     0.357718  0.860981  0.194043    0.345552   9.320282\n",
      "trout     0.193605  0.278596  1.621832    0.300304   9.235664\n"
     ]
    }
   ],
   "source": [
    "num_topwords = 6\n",
    "for c in fieldnames:\n",
    "    print(\"\\nWords with highest scores for {}:\".format(c))\n",
    "    print(wordscores.sort_values(c, ascending=False)[:num_topwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a few funny ones, like \"L\" being the top word in Eco/Geo because it often stands for Carl Linnaeus, the founder of modern taxonomy, in scientific names of species, but overall the scores seem pretty reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, computing the scores for each thesis title, by just taking the average of the scores of individual words in the title. This takes a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing title scores for Social.\n",
      "Computing title scores for Science.\n",
      "Computing title scores for Biomed.\n",
      "Computing title scores for Humanities.\n",
      "Computing title scores for Eco/Geo.\n"
     ]
    }
   ],
   "source": [
    "for c in fieldnames:\n",
    "    print(\"Computing title scores for {}.\".format(c))\n",
    "    # The python sum function is significantly faster here than a sub-DataFrame.sum().\n",
    "    df[c] = df[\"Words\"].apply(\n",
    "        lambda ws: 0.0\n",
    "        if not ws\n",
    "        else sum(wordscores.loc[w, c] for w in ws) / len(ws)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trends in academic fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how the scores are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Unknown property nrows",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-e5da9299b68c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnum_fields\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_fields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_fields\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfieldnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msubplot\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1083\u001b[0m     \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1084\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1085\u001b[0m     \u001b[0mbbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbbox\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m     \u001b[0mbyebye\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36madd_subplot\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1365\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msubplot_class_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprojection_class\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msca\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_subplots.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, *args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# _axes_class is set in the subplot_class_factory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_axes_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;31m# add a layout box to this, for both the full axis, and the poss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# of the axis.  We need both because the axes may become smaller\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/axes/_base.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fig, rect, facecolor, frameon, sharex, sharey, label, xscale, yscale, **kwargs)\u001b[0m\n\u001b[1;32m    504\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_yscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxaxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, props)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meventson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_update_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meventson\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 916\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_update_property\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/matplotlib/artist.py\u001b[0m in \u001b[0;36m_update_property\u001b[0;34m(self, k, v)\u001b[0m\n\u001b[1;32m    910\u001b[0m                 \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'set_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown property %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Unknown property nrows"
     ]
    }
   ],
   "source": [
    "num_fields = len(fieldnames)\n",
    "fig, axes = plt.subplot(nrows=num_fields, ncols=num_fields, figsize=(7, 7))\n",
    "for ix, cx in enumerate(fieldnames):\n",
    "    for iy, cy in enumerate(fieldnames):\n",
    "        df[cx, cy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8efe5c19f048ef8704d1931d23ebdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95ed7e503a348239e9da5147dcbe9f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da18e8644f14419f82f40236035649ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d885cfc4a5d47c8baa8b2e71f52f870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8d749c4ab64f45842589a4128a4a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for c in fieldnames:\n",
    "    plt.figure()\n",
    "    sns.kdeplot(df[c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/matplotlib/pyplot.py:514: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  max_open_warning, RuntimeWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63618736ffaf4a86bc3ce087ae2552ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.PairGrid at 0x7f0ec1ed2ed0>"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bins = 70\n",
    "maxscore = 5\n",
    "\n",
    "\n",
    "def hist2d(x, y, color, **kwargs):\n",
    "    cmap = sns.light_palette(color, as_cmap=True)\n",
    "    plt.hist2d(x, y, bins=(bins, bins), cmap=cmap, **kwargs)\n",
    "\n",
    "\n",
    "def hist(x, color, **kwargs):\n",
    "    plt.hist(x, bins=bins, **kwargs)\n",
    "\n",
    "g = sns.PairGrid(df[fieldnames], height=1.6)\n",
    "g.map_offdiag(hist2d)\n",
    "g.map_diag(hist)\n",
    "g.set(ylim=(0, maxscore), xlim=(0, maxscore))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae3664b2a78f4d619be92d51ff49dfe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed04d81055d5445e8a62ce93e616003c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e39bc81ebd45f8b1113c53a6638be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a2f0e1ae884faa946dfecd2183b7bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c93b6d572fa4d86b94094c6ae5ac8fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for c in fieldnames:\n",
    "    plt.figure()\n",
    "    df[c].hist(bins=200)\n",
    "    plt.title(c)\n",
    "    plt.xlim(0.0, 7.0)\n",
    "    if c == \"Eco/Geo\":\n",
    "        plt.ylim(0, 25000)\n",
    "    elif c == \"Humanities\":\n",
    "        plt.ylim(0, 35000)\n",
    "    else:\n",
    "        plt.ylim(0, 13000)\n",
    "    plt.ylabel(\"Number of theses\")\n",
    "    plt.xlabel(\"Score for {}\".format(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are histograms of how many theses get which score for the various fields. Note that the first three have the same scale for the vertical axis, but Eco/Geo does not.\n",
    "\n",
    "The first peak in each one of these is not the most interesting part. Since we smudged the boundaries of fields by adding neighbouring edge weights when scoring words, most words have a non-zero score for all the fields. The first peaks then represent a sort of baseline value, of what's a usual score for a thesis that isn't really in the given field. The fact that the positions of the first peaks for all four fields roughly line up is an indicator that our normalization of the scores was somewhat successful, and there's meaning to comparing scores across fields.\n",
    "\n",
    "The tails, from score 1.5 or so right-wards, are where the action is. Eco/Geo has a pretty smoothly declining distribution and small values overall, since it is by far the least popular of the fields, presumably because it is the most specific. Other than that, the main difference between fields here is that whereas Science and Hum/Soc have a big solid bulge around scores 2.0 - 4.0, Biomed in contrast has a much thinnes but longer tail reaching all the way to scores larger than 5.0. Roughly speaking this means that a thesis either is or isn't Biomed, with less middle ground compared to the Hum/Soc and Science scores that leave more ambiguity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For these field labels to be good labels, they shouldn't have much correlation between them, at least not positive correlation, so let's check that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = df[wordscores.columns].corr()\n",
    "plt.matshow(corrmat)\n",
    "print(corrmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we are doing well here. Having a high Hum/Soc score makes it unlikely to also score highly on Biomed or Science, but otherwise the different labels are pretty independent. This fits at least my intuitive expectations, although I'm a little surprised that among the more Nature-oriented fields of Biomed, Science and Eco/Geo the little correlation that exists is negative: I would have guessed topics like chemistry and population biology to cause some mixing between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being pretty happy with our scoring method, lets see what trends we can detect over time and institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, here are the mean scores of theses published in each year, over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanscores_by_year = df.groupby([\"Date\"]).mean().fillna(0.0)\n",
    "meanscores_by_year.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's quite a lot going on here. Science and Hum/Soc seem to be at odds, with Hum/Soc having a V-shaped development of popularity first declining until around 1970 and then slowly growing back again to its former glory, and Science following roughly a reverse of this. Especially notable is the huge bump in Science scores in the 60s, before the steady decline sets in. Biomed has the steadiest trend, with slow growth overall, whereas Eco/Geo remains pretty stable until the early 80s, after which it sets into a steady decline, that seems to have leved off lately.\n",
    "\n",
    "To put it more poetically, the one feature to highlight is that the latter half of the last century was heavily dominated by hard sciences and engineering, at the expense of humanities and social sciences, which have then recovered and regained their past status. It would be very interesting to try to compare this to other societal and academic trends, such as gender distributions of PhD graduates, changes in academic funding in the UK, and data on students at earlier stages in their education, such as the popularity of different undergraduate majors and changes in primary and secondary education curricula. For the moment, though, this remains work to be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can observe academic trends for individual institutions. For instance, here are the same plots as above, but specific to some of the golden triangle universities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schools = [\n",
    "    \"University of Oxford\",\n",
    "    \"University of Cambridge\",\n",
    "    \"Imperial College London\",\n",
    "    \"London School of Economics and Political Science (University of London)\",\n",
    "]\n",
    "meanscores_by_yearandinst = (\n",
    "    df.groupby([\"Institution\", \"Date\"])\n",
    "    .mean()\n",
    "    .unstack(\"Institution\")\n",
    "    .fillna(0.0)\n",
    "    .reorder_levels([1, 0], axis=1)\n",
    ")\n",
    "ylims = (-0.05, 4.05)\n",
    "for school in schools:\n",
    "    plt.figure()\n",
    "    lines = plt.plot(meanscores_by_yearandinst[school])\n",
    "    plt.title(school)\n",
    "    plt.ylim(*ylims)\n",
    "    plt.legend(lines, meanscores_by_yearandinst[school].columns)\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Mean field score of thesis titles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardly surprisingly, London School of Economics is heavily leaning towards humanities and social sciences, and Imperial lives up to its reputation of being a science school first and foremost. The above also provides evidence for the wide-spread view that of the Oxbridge two, Cambridge is the more science heavy one, whereas Oxford leans towards Hum/Soc, although the difference isn't huge, at least not by our metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.edges[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# - Abnormal titles with far-apart words (inverse weights, all_pairs_dijkstra_path_length).\n",
    "# - Do a visualization of the graph?\n",
    "# - We vs I\n",
    "# - Can I find correlations between words in titles at institutions, and their university rankings? Suggestion: https://www.leidenranking.com/ranking/2019/chart\n",
    "# - Possible hypotheses to test: Humanities words have less peaked popularity profiles.\n",
    "# - Credit feedback\n",
    "# - Mention more robust comparison of community finding algorithms could be done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if you dig into this dataset yourself and find interesting features I've missed, please let me know: markus@mhauru.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = \"finite\"\n",
    "print(wordscores.loc[word, :])\n",
    "print()\n",
    "[print(w) for w in get_related_words(word, 5)]\n",
    "print()\n",
    "[print(w) for w in get_example_titles(word, 5)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
