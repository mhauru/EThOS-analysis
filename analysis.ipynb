{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring trends in UK academia using PhD thesis metadata\n",
    "#### Markus Hauru, January 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The British Library operates its [Electronic Theses Online Service](https://ethos.bl.uk) or EThOS, that keeps track of PhD theses accepted at UK higher education institutions. The Library also makes public a [data set](https://data.bl.uk/ethos/) of metadata for all theses in their catalogue. Quoting their website,\n",
    "\n",
    ">The EThOS dataset lists virtually all UK doctoral theses ever awarded, some 500,000 dating back to 1787. All UK HE institutions are included, but we estimate records are missing for around 10,000 titles (2%).\n",
    "\n",
    "The latest version of the dataset is from March 2018. The data includes the title, year, author, and institution for each thesis, as well as a link to a full record which may or may not include things like keywords or access to full texts.\n",
    "\n",
    "In this notebook I explore this EThOS dataset to identify historical trends in UK academia. The notebook is structured roughly as follows:\n",
    "* Prepare the data for analysis\n",
    "* Study the number of PhDs produced as a function of institution and year\n",
    "* Study trends in the popularities of individual words appearing in thesis titles\n",
    "* Use a co-occurrence graph of words in thesis titles to detect academic fields, and score thesis titles on which field(s) they seem to belong to\n",
    "* Use the above to study trends in the popularity of different academic fields\n",
    "\n",
    "On the way we will for instance see a dramatic shift in the structure of UK academia in the 60s and 70s, look at some of the trendiest academic buzzwords of each decade since the 30s, and find out whether Cambridge really is more science-oriented than Oxford.\n",
    "\n",
    "While I hope the findings here are of interest in themselves, this notebook is also an exercise for myself, to learn some basic exploratory data science tools and techniques, like basics of pandas, some network analysis, and new visualization techniques.\n",
    "\n",
    "Many thanks go to Henri Seijo for some great feedback on this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up and preparing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, some imports of python libraries we'll be needing, and loading the data file into a pandas DataFrame. The script will automatically download the data file into the current working directory if it isn't there yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import operator as opr\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import community  # Network community finding\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.colors\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For pretty and interactive matplotlib plots in Jupyter Lab.\n",
    "%matplotlib widget\n",
    "# Set a consistent plotting style across the notebook using Seaborn.\n",
    "sns.set_style(\"darkgrid\")\n",
    "sns.set_context(\"notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = \"EThOSCSV_201803.csv\"\n",
    "if not os.path.isfile(datafile):\n",
    "    # Download and/or unzip the data file from the EThOS website.\n",
    "    # We need a couple more imports for this.\n",
    "    import zipfile\n",
    "\n",
    "    datazip = \"EThOSCSV_201803.zip\"\n",
    "    if not os.path.isfile(datazip):\n",
    "        import requests\n",
    "\n",
    "        dataurl = \"https://data.bl.uk/ethos/EThOSCSV201803.zip\"\n",
    "        with requests.Session() as s:\n",
    "            headers = {\n",
    "                \"User-agent\": \"Mozilla/5.0 (X11; Linux i686; rv:64.0) Gecko/20100101 Firefox/64.0\"\n",
    "            }\n",
    "            r = s.get(dataurl, headers=headers)\n",
    "            with open(datazip, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "    with zipfile.ZipFile(datazip, \"r\") as z:\n",
    "        z.extractall(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(datafile, encoding=\"ISO-8859-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get an idea of what the rows in the DataFrame look like, here are some samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Author</th>\n",
       "      <th>Institution</th>\n",
       "      <th>Date</th>\n",
       "      <th>Qualification</th>\n",
       "      <th>EThOS URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Computation and measurement of turbulent flow ...</td>\n",
       "      <td>Loizou, Panos A.</td>\n",
       "      <td>University of Manchester</td>\n",
       "      <td>1989.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Prolactin and growth hormone secretion in norm...</td>\n",
       "      <td>Prescott, R. W. G.</td>\n",
       "      <td>University of Newcastle upon Tyne</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Influence of strain fields on flame propagation</td>\n",
       "      <td>Mendes-Lopes, J. M. C.</td>\n",
       "      <td>University of Cambridge</td>\n",
       "      <td>1983.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Connectivity, flow and transport in network mo...</td>\n",
       "      <td>Robinson, Peter Clive</td>\n",
       "      <td>University of Oxford</td>\n",
       "      <td>1984.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The theory and implementation of a high qualit...</td>\n",
       "      <td>Lower, K. N.</td>\n",
       "      <td>University of Bristol</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>Thesis (Ph.D.)</td>\n",
       "      <td>http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title                  Author  \\\n",
       "0  Computation and measurement of turbulent flow ...        Loizou, Panos A.   \n",
       "1  Prolactin and growth hormone secretion in norm...      Prescott, R. W. G.   \n",
       "2    Influence of strain fields on flame propagation  Mendes-Lopes, J. M. C.   \n",
       "3  Connectivity, flow and transport in network mo...   Robinson, Peter Clive   \n",
       "4  The theory and implementation of a high qualit...            Lower, K. N.   \n",
       "\n",
       "                         Institution    Date   Qualification  \\\n",
       "0           University of Manchester  1989.0  Thesis (Ph.D.)   \n",
       "1  University of Newcastle upon Tyne  1983.0  Thesis (Ph.D.)   \n",
       "2            University of Cambridge  1983.0  Thesis (Ph.D.)   \n",
       "3               University of Oxford  1984.0  Thesis (Ph.D.)   \n",
       "4              University of Bristol  1985.0  Thesis (Ph.D.)   \n",
       "\n",
       "                                           EThOS URL  \n",
       "0  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  \n",
       "1  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  \n",
       "2  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  \n",
       "3  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  \n",
       "4  http://ethos.bl.uk/OrderDetails.do?uin=uk.bl.e...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first observation to make is that the data is remarkably clean. There are a few NaNs that we need to drop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with NaNs: 13\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of rows with NaNs: {}\".format(df.isnull().any(axis=1).sum()))\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but with that done, there's little else to do. The Qualification column holds some oddities and misunderstandings, and there are few suspiciously short titles (my favourite being the 1977 thesis called \"Beds\"), but other than that, things seem in order. Most notably, the Institution and Date fields, which we'll be getting a lot of mileage from, seem spotless, without even a single typoed university name. The Dates are all just years, so I convert them to an integer type and rename the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Date\"] = df[\"Date\"].astype(np.int64)\n",
    "df.rename(columns={\"Date\": \"Year\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is of some concern are the 10,000 or so theses that the British Library estimates are missing from the data set. Nowadays that's about a year's worth of theses, but it's also roughly the total number of theses in the data from before 1956. Now clearly some of the missing ones are from the last few years: Even though the data set is from 2018, the number of theses in it drops sharply after 2015. But there's probably a significant number missing from the early years as well, especially considering issues of record keeping. To illustrate the late and early parts of the data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesiscounts_by_year = df.Year.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3122e2c3222145619e445f62f87d4aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(7, 3))\n",
    "ax1 = fig.add_subplot(1, 2, 1)\n",
    "ax1.plot(thesiscounts_by_year.loc[2005:])\n",
    "plt.ylabel(\"Number of PhD theses\")\n",
    "plt.xlabel(\"Year\")\n",
    "\n",
    "ax2 = fig.add_subplot(1, 2, 2)\n",
    "ax2.plot(thesiscounts_by_year.loc[:1940])\n",
    "plt.xlabel(\"Year\")\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, to get a more reliable data set, I chop off parts from both ends. 2015 is clearly the natural late cutoff, since I doubt the PhD output has actually collapsed in the last few years. At the early end, before the end of WW1 the sample sizes for each year are clearly quite low, and based on this, as well as some odd features of the data pre-WW1, I've chosen to set the early cutoff at 1925."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "late_cutoff = 2015\n",
    "early_cutoff = 1925\n",
    "thesiscounts_by_year = thesiscounts_by_year.loc[early_cutoff:late_cutoff]\n",
    "df = df[(early_cutoff <= df.Year) & (df.Year <= late_cutoff)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also drop the columns we won't be using for anything. Makes for nicer reading when we print out slices of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([\"Qualification\", \"EThOS URL\"], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When and where: Number of theses per year and institution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that basic setup out of the way, let's get into this.\n",
    "\n",
    "First, a look at the number of PhD theses accepted in the UK over our chosen time period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9aff2a046064d06aadcb23d56730e7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.semilogy(thesiscounts_by_year)\n",
    "ax.set_ylabel(\"Number of PhD theses.\")\n",
    "ax.set_xlabel(\"Year\")\n",
    "ax.xaxis.set_major_locator(plt.MultipleLocator(10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the logarithmic vertical axis. The two word summary here would clearly be \"exponential growth\". In 1925 a bit more than a hundred PhDs were awarded, and now we are approaching 20 000 per year. The most notable features on the way are\n",
    "* a huge dip during WW2 in the middle of the otherwise steady exponential period from 1925 to 1960. (In the early-times plot a couple of cells above you can also see a dip during WW1, although the signal there is more noisy.) There's also a bump a few years after WW2, presumably from people who postponed the start of their studies until after the war and are then graduating in large batches.\n",
    "* a period of even faster exponential growth in the 1960-1975 window. One explanation for this is baby boomers hitting typical PhD age (compare the above to a UK birth rate plot [here](https://www.ons.gov.uk/peoplepopulationandcommunity/birthsdeathsandmarriages/livebirths/bulletins/birthsummarytablesenglandandwales/2017) and shift by 25 or so years), but probably the most important driver of this growth is the huge structural change in academia that we'll see in a moment.\n",
    "* a more moderate but steady pace from roughly 1975 onwards.\n",
    "\n",
    "Note that when I talk about rate or pace of growth here I'm talking in multiplicative terms, meaning for instance that since 1975 the number of new PhDs has been growing by a roughly constant _percentage_ per year, not a constant number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's look at the contributions of different institutions to the total output of PhDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesiscounts_by_yearandinst = (\n",
    "    df.groupby([\"Institution\", \"Year\"])\n",
    "    .size()\n",
    "    .unstack(\"Institution\")\n",
    "    .fillna(0.0)\n",
    ")\n",
    "# Sort institutions based on PhD output in the last year of the time window.\n",
    "thesiscounts_by_yearandinst = thesiscounts_by_yearandinst.sort_values(\n",
    "    late_cutoff, axis=\"columns\", ascending=False\n",
    ")\n",
    "# Compute the proportion of PhDs produced by each institution by year.\n",
    "instratios_by_year = thesiscounts_by_yearandinst.divide(\n",
    "    thesiscounts_by_year, axis=\"index\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1690b57dae42248892f00c8977f974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can't show the full legend with dozens of entries, so we restrict to a few\n",
    "# of the most important ones.\n",
    "num_legendentries = 10\n",
    "num_entries = len(instratios_by_year.columns)\n",
    "\n",
    "# Get the names and indices of the institutions that will cover the largest\n",
    "# area in this plot. Sort them by index.\n",
    "instratio_sums = instratios_by_year.sum().sort_values(ascending=False)\n",
    "top_insts = instratio_sums.index[:num_legendentries]\n",
    "top_inds = [list(instratios_by_year.columns).index(inst) for inst in top_insts]\n",
    "top_inds, top_insts = zip(*sorted(zip(top_inds, top_insts)))\n",
    "\n",
    "# Pick colors that highlight the entries in the legend.\n",
    "legend_colors = sns.color_palette(\"dark\", n_colors=num_legendentries)\n",
    "# Color all the other entries with the same shade of gray, which we pick to be\n",
    "# one of the highlight colors desaturated.\n",
    "default_color = sns.set_hls_values(legend_colors[1], s=0.0)\n",
    "all_colors = [default_color] * num_entries\n",
    "# Put the highlight colors at the right positions in the list of all colors.\n",
    "for i, c in zip(top_inds, legend_colors):\n",
    "    all_colors[i] = c\n",
    "\n",
    "# Make an exception to the general style by giving this plot ticks.\n",
    "style_dict = {\"xtick.bottom\": True, \"ytick.left\": True}\n",
    "with sns.axes_style(style_dict):\n",
    "    plt.figure(figsize=(9, 5))\n",
    "    lines = plt.stackplot(\n",
    "        instratios_by_year.index,\n",
    "        instratios_by_year.values.T,\n",
    "        colors=all_colors,\n",
    "        lw=0.05,\n",
    "    )\n",
    "    plt.ylabel(\"Proportion of PhDs\")\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylim(0.0, 1.0)\n",
    "    plt.xlim(early_cutoff, late_cutoff)\n",
    "    # Some fancy formatting of the legend. We show the most important legend\n",
    "    # entries in the same order that they appear in in the figure and position\n",
    "    # the legend out of the way.\n",
    "    top_lines = [lines[i] for i in top_inds]\n",
    "    plt.legend(\n",
    "        reversed(top_lines),\n",
    "        reversed(top_insts),\n",
    "        loc=\"lower left\",\n",
    "        bbox_to_anchor=(1.0, 0.0),\n",
    "    )\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot shows the proportion of PhDs coming from different universities, each grey or colored band being one institution. The institutions are ordered by the number of PhDs they produced in 2015, with the largest one being Oxford at the bottom. The colors and the legend highlight a few of the most historically prominent instutions (the ones that get the largest area in the above plot).\n",
    "\n",
    "Here we can see the drastic change in the structure of higher education between roughly 1960 and 1980 that I mentioned above: There's an explosion in the proportion of PhDs coming from small universities. Until 1960 or so Oxford, Cambridge and Edinburgh produced 70-80% of PhDs in the country. By 1980 it was around 20%, and has stayed roughly constant since. They remain the largest PhD factories, but the field has been filled with dozens of smaller institutions.\n",
    "\n",
    "A bit of Wikipedia browsing gives some context for this. During the shift period many new universities were founded (Sussex 1961, York 1963, Warwick 1965, ...), many others expanded greatly, and various colleges and other educational institutions were turned into universities. I'm not a historian, but for some background check Wikipedia on for instance the [University Grant Committee](https://en.wikipedia.org/wiki/University_Grants_Committee_(United_Kingdom)), [Robbins Report](https://en.wikipedia.org/wiki/Robbins_Report), and [Education Act of 1962](https://en.wikipedia.org/wiki/Education_Act_1962). I was previously aware that there was an expansion of higher education in many Western countries around this time, but the magnitude and speed of the shift in the above plot surprised me.\n",
    "\n",
    "There are also some interesting individual stories in this same plot. Oxford used to dominate its rival Cambridge with a doctoral output several times larger until the late 60s, after which the two have been roughly equal. Even Oxford was for a time ecplised by Edinburgh though, which in some years produced more than 40% of all PhDs in the UK. The historical importance of Scotland was further bolstered by Aberdeen, which used to be one of the major players until the 60s, but is now just another mid-sized entry. (It should be said though that I suspect that the early years may suffer from a significant number of missing theses, that may skew the data to some degree. The main qualitative statement probably still stands though.) Imperial has a bulge of activity during the 1960s as it went through [a rapid expansion](https://en.wikipedia.org/wiki/Imperial_College_London#20th_century), a bit before many others followed suit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to underline the fact that the fall in the relative importance of the top few institutions is not at all due to shrinking on their part but merely the growth of others, here are the absolute numbers of PhDs produced by the 4 institutions that contributed the most in 1960."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4b357487ed4f819b2262fd6a16c43b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_insts = 4\n",
    "counts_top = thesiscounts_by_yearandinst.sort_values(\n",
    "    1960, axis=1, ascending=False\n",
    ")\n",
    "counts_top = counts_top.iloc[:, :num_insts]\n",
    "# Prepare to a format that sns.FacetGrid likes.\n",
    "counts_top = (\n",
    "    counts_top.unstack().reset_index().rename(columns={0: \"Number of theses\"})\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    counts_top, col=\"Institution\", col_wrap=2, sharey=True, sharex=True\n",
    ")\n",
    "g.map(plt.plot, \"Year\", \"Number of theses\").set_titles(\"{col_name}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are titles made of?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've only looked at where and when PhD theses have been written. The dataset doesn't hold that much more information to toy with. It doesn't list departments or faculties, keywords or research fields, nor do I have easy access to full texts of the theses. But what it does have is the titles of these theses. Let's see what we can learn from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up the title analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we get to looking at trends in titles, we need to spend a moment setting up some machinery that we can then milk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thesis titles aren't exactly prose. They typically aren't full sentences and their grammatical forms vary wildly, which makes analysis of their linguistic structure tricky and, I would guess, in many cases a bit futile. Unlike with more structured text, almost as much information as in the title itself is contained in just a list of words appearing in the title. Lists of words are also easy to analyse, so we'll go with that.\n",
    "\n",
    "To start, I strip the titles of any punctation and so called stop words like articles and prepositions that don't tell us much, and make everything lower case. I could also do what's called stemming, and collapse for instance \"study\", \"studies\", \"studied\" and \"studying\" all into a single word. I choose not to do this because as we'll see below, different inflections of the same stem word sometimes appear in significantly different kinds of titles. Moreover, with academic vocabulary, doing this properly isn't straight-forward: the stemming algorithm should for instance know to combine \"phenomenon\" and \"phenomena\", but perhaps not \"phenomenal\" or \"phenomenology\", and certainly not \"phenolic\". A quick experiment that I did with one stemmer also suggested that the results we'll be seeing wouldn't be greatly affected.\n",
    "\n",
    "Another choice I make is to remove hyphens as unnecessary punctuation. Here you lose some and you win some. \"Post-modern\" should certainly be grouped together with \"postmodern\", but \"biodiversity-ecosystem\" (which appears in 5 titles) would be better split into two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitstr(s):\n",
    "    \"\"\" For a string, remove most punctuation, lower case, and split into\n",
    "    words. Return the unique words.\n",
    "    \"\"\"\n",
    "    puncts = \"!\\\"&'(),./:;<=>?[\\\\]`{|}-\"\n",
    "    words = s.lower().translate(str.maketrans(\"\", \"\", puncts)).split()\n",
    "    # Remove duplicates. We don't care if a word occurs multiple times in\n",
    "    # the same title.\n",
    "    words = tuple(set(words))\n",
    "    return words\n",
    "\n",
    "\n",
    "def filter_stopwords(words):\n",
    "    \"\"\" Take a list of strings, filter out prepositions, articles, and other\n",
    "    stop words.  We use a list of English stop words found here:\n",
    "    https://www.textfixer.com/tutorials/common-english-words.txt\n",
    "    \"\"\"\n",
    "    # Stop black (the code formatter) from automatically splitting this\n",
    "    # across a gazillion lines with the fmt: off/on.\n",
    "    # fmt: off\n",
    "    stops = [\"a\", \"able\", \"about\", \"across\", \"after\", \"all\", \"almost\", \"also\", \"am\", \"among\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"but\", \"by\", \"can\", \"cannot\", \"could\", \"dear\", \"did\", \"do\", \"does\", \"either\", \"else\", \"ever\", \"every\", \"for\", \"from\", \"get\", \"got\", \"had\", \"has\", \"have\", \"he\", \"her\", \"hers\", \"him\", \"his\", \"how\", \"however\", \"i\", \"if\", \"in\", \"into\", \"is\", \"it\", \"its\", \"just\", \"least\", \"let\", \"like\", \"likely\", \"may\", \"me\", \"might\", \"most\", \"must\", \"my\", \"neither\", \"no\", \"nor\", \"not\", \"of\", \"off\", \"often\", \"on\", \"only\", \"or\", \"other\", \"our\", \"own\", \"rather\", \"said\", \"say\", \"says\", \"she\", \"should\", \"since\", \"so\", \"some\", \"than\", \"that\", \"the\", \"their\", \"them\", \"then\", \"there\", \"these\", \"they\", \"this\", \"tis\", \"to\", \"too\", \"twas\", \"us\", \"wants\", \"was\", \"we\", \"were\", \"what\", \"when\", \"where\", \"which\", \"while\", \"who\", \"whom\", \"why\", \"will\", \"with\", \"would\", \"yet\", \"you\", \"your\"]\n",
    "    # fmt: on\n",
    "    return tuple(word for word in words if word not in stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Words\"] = df[\"Title\"].apply(splitstr).apply(filter_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the title analysis going, we want three different objects:\n",
    "* A DataFrame that lists the total number of titles in which each word appears.\n",
    "* The same thing but now per year. (We could do per institution as well, but let's leave that for later.)\n",
    "* A so called co-occurrence graph (or network), i.e. a weighted graph where nodes are words and edges tell us which words appear together in titles and how often.\n",
    "\n",
    "First, the easy bit, just counting word occurrences per year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounts_by_year = (\n",
    "    df.groupby(\"Year\")[\"Words\"]\n",
    "    .apply(lambda x: pd.Series(np.concatenate(x.tolist())).value_counts())\n",
    "    .unstack(\"Year\")\n",
    "    .fillna(0.0)\n",
    "    .T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_wordcounts = wordcounts_by_year.sum(axis=0)\n",
    "# Sort both total_wordcounts and wordcounts_by_year to have the most common\n",
    "# words first.\n",
    "order = (-total_wordcounts).argsort()\n",
    "total_wordcounts = total_wordcounts[order]\n",
    "allwords = total_wordcounts.index\n",
    "wordcounts_by_year = wordcounts_by_year.reindex(allwords, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also compute the occurrences of each word per 1000 theses, in each given year, to measure their relative popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordratios_by_year = 1000 * wordcounts_by_year.divide(\n",
    "    thesiscounts_by_year, axis=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making the co-occurrence graph requires some thought. First off, we've got around 190,000 distinct words in about 450,000 titles (academics like their jargon). However, only 20,000 or so of them appear in more than 10 thesis titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words: 190252. Number of theses: 451755. Number of words with more than 10 occurrences: 21584.\n"
     ]
    }
   ],
   "source": [
    "wordcount_cutoff = 10\n",
    "print(\n",
    "    \"Number of unique words: {}. Number of theses: {}. Number of words with more than {} occurrences: {}.\".format(\n",
    "        len(total_wordcounts),\n",
    "        len(df),\n",
    "        wordcount_cutoff,\n",
    "        (total_wordcounts > wordcount_cutoff).sum(),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus I choose to restrict the graph to this subset of words, since it makes it computationally much lighter to handle, without causing much of a loss: Words that only occur in a handful of thesis titles probably wouldn't add much to the analysis anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainwords = allwords[total_wordcounts > wordcount_cutoff]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note also that when making word co-occurrence graphs of texts, co-occurrences are often given more weight if the words are next to each rather than just in the same sentence. I don't make this distinction since the titles are pretty short anyway, and word orders in titles can be unusual, making proximity less relevant. I also don't differentiate between the same word appearing in a title once or multiple times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the graph/network, I use the [NetworkX](https://networkx.github.io/) package. It represents graphs as adjacency lists, which suits our quite sparse co-occurrence graph well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooc_graph = nx.Graph()\n",
    "cooc_graph.add_nodes_from(mainwords)\n",
    "for words in df[\"Words\"]:\n",
    "    words = tuple(filter(lambda w: w in mainwords, words))\n",
    "    # Loop over all pairs of two words `wi` and `wj` in `words`, but\n",
    "    # so that `(wi, wi)` are not included, and if `(wi, wj)` is included\n",
    "    # then `(wj, wi)` is not.\n",
    "    for i in range(len(words)):\n",
    "        wi = words[i]\n",
    "        # The second loop starts from i+1 to avoid double-counting.\n",
    "        for j in range(i + 1, len(words)):\n",
    "            wj = words[j]\n",
    "            if wi in cooc_graph[wj]:\n",
    "                # This edge already exists, increment the weight\n",
    "                cooc_graph[wi][wj][\"weight\"] += 1.0\n",
    "            else:\n",
    "                # This edge doesn't yet exist, create it.\n",
    "                cooc_graph.add_edge(wi, wj, weight=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have it now, the edge weights of the graph just count the number of theses in which both words appear. This isn't quite what we want, since it's dominated by commonly occuring words. There are several different ways we could normalize the weights. To make a long story short, a choice that I've found works well for what I want to do with this graph later is normalizing the edge between A and B by average of the frequencies of A and B. This is symmetric (the graph remains undirected) and roughly speaking gives a large weight for edges that connect two words that appear approximately equally frequently and often together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w1, w2 in cooc_graph.edges:\n",
    "    avg_count = (total_wordcounts[w1] + total_wordcounts[w2]) / 2.0\n",
    "    cooc_graph[w1][w2][\"weight\"] /= avg_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a few convenient functions for studying individual words: Getting example thesis titles with a given word in it, getting related words, and outputting a summary of a word (popularity plot, related words, examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_example_titles(word, n=1):\n",
    "    \"\"\" For a given word `w`, return a tuple of `n` randomly picked thesis\n",
    "    titles that have `w` in it.\n",
    "    \"\"\"\n",
    "    titles_with_word = df[df[\"Words\"].apply(lambda x: word in x)][\"Title\"]\n",
    "    n = min(n, len(titles_with_word))\n",
    "    examples = tuple(titles_with_word.sample(n).values)\n",
    "    return examples\n",
    "\n",
    "\n",
    "def get_related_words(w, n=5, weights=True):\n",
    "    \"\"\" For a given word `w`, get the `n` words with heaviest edges connected\n",
    "    to `w` in the co-occurrence graph.  Returns a list of tuples\n",
    "    `(neighbour_word, weight)`, sorted by `weight`, or alternatively just a\n",
    "    list of `neighbour_word`s if `weights=False`.\n",
    "    \"\"\"\n",
    "    if w not in cooc_graph:\n",
    "        # This word is not in the co-occurrence network.\n",
    "        return ()\n",
    "    node = cooc_graph[w]\n",
    "    neighbourlist = ((k, v[\"weight\"]) for k, v in node.items())\n",
    "    neighbourlist = sorted(neighbourlist, key=opr.itemgetter(1), reverse=True)\n",
    "    neighbourlist = neighbourlist[:n]\n",
    "    if not weights:\n",
    "        neighbourlist = [w[0] for w in neighbourlist]\n",
    "    return neighbourlist\n",
    "\n",
    "\n",
    "def output_word_summary(w, n_related=3, n_examples=3, plot=True):\n",
    "    \"\"\" Print and plot (if `plot=True`) a summary of data for the word `w`.\n",
    "    \"\"\"\n",
    "    print(\"Word: {}\".format(w))\n",
    "    if n_related > 0:\n",
    "        print(\"Related words:\")\n",
    "        for neighbour in get_related_words(w, n=n_related):\n",
    "            print(\" {}: {}\".format(*neighbour))\n",
    "    if n_examples > 0:\n",
    "        print(\"Example titles:\")\n",
    "        for title in get_example_titles(w, n=n_examples):\n",
    "            print(\" {}\".format(title))\n",
    "    if plot:\n",
    "        fig, ax1 = plt.subplots(figsize=(5, 3))\n",
    "        ax1.plot(wordcounts_by_year[w], color=\"black\")\n",
    "        title = \"Number and proportion of theses\\nwith '{}' in the title\".format(\n",
    "            w\n",
    "        )\n",
    "        ax1.set_title(title)\n",
    "        ax1.set_ylabel(\"Absolute number (black)\".format(w))\n",
    "        ax1.set_xlabel(\"Year\")\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.plot(wordratios_by_year[w], color=\"darkred\")\n",
    "        ax2.set_ylabel(\"Per 1000 theses (red)\".format(w))\n",
    "        fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buzzwords from the past"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all that set-up out of the way, let's see if these titles hold some interesting. First off, what are the most commonly occuring words in thesis titles, and how does their popularity vary over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba44fa454b88490e930bd53f95170d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Word: study\n",
      "Related words:\n",
      " case: 0.2097815470376635\n",
      " comparative: 0.1153317337503033\n",
      " development: 0.05929317390646284\n",
      "Example titles:\n",
      " Communicating philosophically and theologically : a study of the dialogue between the mainstream Reformed and Edwardian traditions of the seventeenth to nineteenth centuries concerning sin and salvation\n",
      " Study of coryza of the domestic fowl, with special reference to its bacteriology\n",
      " Labour scarcity and uneven income distribution, a case study in regional development strategy with special reference to the Sudan\n",
      "--------------------------------------------------\n",
      "Word: studies\n",
      "Related words:\n",
      " structural: 0.09210299541553205\n",
      " synthesis: 0.050235956766631146\n",
      " synthetic: 0.048787855902122194\n",
      "Example titles:\n",
      " Studies on the proetins of rat liver heterogeneous nuclear ribonucleo-protein particles\n",
      " Spectroscopic and imaging studies of nightglow variations\n",
      " Studies on lactational performance in relation to energy intake\n",
      "--------------------------------------------------\n",
      "Word: development\n",
      "Related words:\n",
      " system: 0.06787693205016039\n",
      " application: 0.06536252134761683\n",
      " role: 0.0598641998380365\n",
      "Example titles:\n",
      " The path to HRD : an investigation of training and development practices in the Libyan manufacturing sector in 21st century\n",
      " Why Donegal slept : the development of Gaelic games in Donegal, 1884-1934\n",
      " The development of real-time distributed hybrid testing for earthquake engineering\n",
      "--------------------------------------------------\n",
      "Word: analysis\n",
      "Related words:\n",
      " using: 0.06964454230890217\n",
      " data: 0.06052919308987535\n",
      " functional: 0.053515590100955954\n",
      "Example titles:\n",
      " Aspects of sentence analysis in the Arabic linguistic tradition, with particular reference to ellipsis\n",
      " Probabilistic analysis of interference between sequential construction activities\n",
      " Speciation analysis of aluminum complexes with neurotransmitters in biological media\n"
     ]
    }
   ],
   "source": [
    "num_topwords = 4\n",
    "topwords = allwords[:num_topwords]\n",
    "topword_ratios = wordratios_by_year.loc[:, topwords]\n",
    "# Prepare to a format that sns.FacetGrid likes.\n",
    "topword_ratios = (\n",
    "    topword_ratios.unstack()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"Occurrences per 1000 theses\", \"level_0\": \"Word\"})\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    topword_ratios, col=\"Word\", col_wrap=2, sharex=True, sharey=True,\n",
    ")\n",
    "g.map(plt.plot, \"Year\", \"Occurrences per 1000 theses\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "for w in topwords:\n",
    "    print(\"-\" * 50)\n",
    "    output_word_summary(w, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing too exciting here: Generic terms that appear in all kinds of titles. Both \"study\" and \"studies\" have clearly fallen into relative disuse since the 70s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much more interesting than the most popular words are those whose popularity has been fleeting. One could get at this in a million different ways, but here's one that I found interesting. For each decade, find the top 3 words whose popularity that decade was the most disproportionately large compared to their popularity over all time. In other words, rank words by number of occurrences in a given decade, divided by total number of occurrences. To not have this be dominated by words occurring in only a handful of theses, I further restrict to words with at least 100 occurrences in total. I'll also print for each word some of the commonly co-occurring words, to give some context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1930s:\n",
      "notes            ['introduction', 'glossary', 'edited']\n",
      "solvents         ['eutectic', 'nonpolar', 'nonaqueous']\n",
      "prognosis        ['diagnosis', 'aetiology', 'infarction']\n",
      "-------------------------------------------------------------------------------\n",
      "1940s:\n",
      "substances       ['humic', 'pectic', 'psychoactive']\n",
      "constitution     ['alloys', 'constitutional', 'discursive']\n",
      "cases            ['hundred', 'review', 'report']\n",
      "-------------------------------------------------------------------------------\n",
      "1950s:\n",
      "radioactive      ['disposal', 'tracer', 'waste']\n",
      "substances       ['humic', 'pectic', 'psychoactive']\n",
      "constituents     ['nitrogenous', 'milk', 'leaves']\n",
      "-------------------------------------------------------------------------------\n",
      "1960s:\n",
      "elementary       ['particles', 'particle', 'physics']\n",
      "polysaccharides  ['algal', 'pectic', 'enzymic']\n",
      "bubble           ['chamber', 'columns', 'coalescence']\n",
      "-------------------------------------------------------------------------------\n",
      "1970s:\n",
      "gevc             ['kp', '10', '16']\n",
      "prostaglandins   ['parturition', 'leucocytes', 'menstruation']\n",
      "ultrastructure   ['cytochemistry', 'histochemistry', 'cytology']\n",
      "-------------------------------------------------------------------------------\n",
      "1980s:\n",
      "microprocessor   ['pwm', 'inverter', 'drive']\n",
      "monoclonal       ['antibodies', 'antibody', 'antigens']\n",
      "diagenesis       ['sedimentology', 'sandstones', 'jurassic']\n",
      "-------------------------------------------------------------------------------\n",
      "1990s:\n",
      "objectoriented   ['database', 'databases', 'query']\n",
      "atm              ['traffic', 'congestion', 'switch']\n",
      "lipoprotein      ['lipase', 'density', 'cholesterol']\n",
      "-------------------------------------------------------------------------------\n",
      "2000s:\n",
      "globalisation    ['era', 'firmlevel', 'olympic']\n",
      "internet         ['banking', 'online', 'protocol']\n",
      "ict              ['adoption', 'teachers', 'technology']\n",
      "-------------------------------------------------------------------------------\n",
      "2010s:\n",
      "graphene         ['epitaxial', 'nanoelectronics', 'nanotubes']\n",
      "mindfulness      ['selfcompassion', 'meditation', 'dispositional']\n",
      "resilience       ['socialecological', 'disaster', 'vulnerability']\n",
      "-------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "num_topwords = 3\n",
    "occurrence_cutoff = 100\n",
    "notrare_words = total_wordcounts > occurrence_cutoff\n",
    "for start_year in range(1930, 2020, 10):\n",
    "    end_year = start_year + 10\n",
    "    print(\"{}s:\".format(start_year))\n",
    "    decade_top_words = (\n",
    "        wordcounts_by_year.loc[start_year:end_year, notrare_words].sum()\n",
    "        / total_wordcounts\n",
    "    ).sort_values(ascending=False)\n",
    "    for w in decade_top_words.index[:num_topwords]:\n",
    "        neighbours = get_related_words(w, n=3, weights=False)\n",
    "        print(\"{:15}  {}\".format(w, neighbours))\n",
    "    print(\"-\" * 79)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 30s, 40s, and 50s look quite similar, with pretty generic terminology, especially science terminology, trending high, but from that point on things get more specialised and interesting. Plenty of particle physics terminology is featured in the 60s and 70s (a bubble chamber is a type of particle physics experiment and GeV/c, gigaelectronvolts per speed of light, is a a unit for momentum in particle physics), when new particles were constantly being discovered. The 80s, 90s and 2000s have a lot of computer science and communication technology words in the lead (ATM in most cases stands for asynchronous transfer mode). They give way to \"mindfulness\" and \"resilience\" in the 2010s, nicely reflecting changing cultural themes and attitudes in the past decade. Many of the words are also directly related to breakthroughs that generated flurries of research activity in their wake, such as monoclonal antibodies and graphene, which both lead to a Nobel Prize within a few years of discovery.\n",
    "\n",
    "Notable is also the relative lack of social sciences and humanities vocabulary. We'll come back to this later when studying popularities of academic fields.\n",
    "\n",
    "The prospective rabbit holes here are endless, but just for highlighting, here's a bit more detail on a few select words from the above list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45f812b369a6408abdd392947e5eaf25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Word: bubble\n",
      "Related words:\n",
      " chamber: 0.23267326732673269\n",
      " columns: 0.0691358024691358\n",
      " coalescence: 0.060836501901140684\n",
      "Example titles:\n",
      " An examination of bubble dynamics and double metastability within an improved glass Berthelot tube\n",
      " Bubble growth and nucleate pool boiling in liquid nitrogen, argon, and their mixtures\n",
      " Gas hold-up and mixing in bubble columns containing two or more phases\n",
      "--------------------------------------------------\n",
      "Word: diagenesis\n",
      "Related words:\n",
      " sedimentology: 0.14255765199161424\n",
      " sandstones: 0.10810810810810811\n",
      " jurassic: 0.07514450867052024\n",
      "Example titles:\n",
      " The diagenesis of some Lower Carboniferous Oolitic Limestones from South Wales\n",
      " Depositional environments and diagenesis of the Cenozoic sediments of the Almazan Basin, North-East Spain\n",
      " Aspects of sedimentary facies and diagenesis in limestone-shale formations of the (Middle Jurassic) Great Estuarine Group, Inner Hebrides\n",
      "--------------------------------------------------\n",
      "Word: globalisation\n",
      "Related words:\n",
      " era: 0.04203152364273205\n",
      " firmlevel: 0.03319502074688797\n",
      " olympic: 0.028169014084507043\n",
      "Example titles:\n",
      " To what extent is globalisation creating forms of neo-colonialism within contemporary visual culture?\n",
      " Globalisation and ethnic integration in corporate Malaysia : the case of public-listed companies in the Klang Valley and Penang\n",
      " Anglicisms, globalisation and performativity in Japanese popular culture\n",
      "--------------------------------------------------\n",
      "Word: mindfulness\n",
      "Related words:\n",
      " selfcompassion: 0.08974358974358974\n",
      " meditation: 0.06666666666666667\n",
      " dispositional: 0.03125\n",
      "Example titles:\n",
      " Inquiry into shame : exploring mindfulness, self-compassion, acceptance, and mind-wandering as methods of shame management\n",
      " Mindfulness and non-clinical paranoia\n",
      " Investigation of a developmental account of self-compassion : the result of parental behaviour during childhood and the potential roles of attachment and mindfulness\n"
     ]
    }
   ],
   "source": [
    "selectwords = [\"bubble\", \"diagenesis\", \"globalisation\", \"mindfulness\"]\n",
    "selectword_ratios = wordratios_by_year.loc[:, selectwords]\n",
    "# Prepare to a format that sns.FacetGrid likes.\n",
    "selectword_ratios = (\n",
    "    selectword_ratios.unstack()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"Occurrences per 1000 theses\", \"level_0\": \"Word\"})\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    selectword_ratios, col=\"Word\", col_wrap=2, sharex=True, sharey=False,\n",
    ")\n",
    "g.map(plt.plot, \"Year\", \"Occurrences per 1000 theses\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "for w in selectwords:\n",
    "    print(\"-\" * 50)\n",
    "    output_word_summary(w, plot=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying fields of research from titles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Individual words like those above tell us stories of individual scientific discoveries and hot topics, but they have their limits if we want to study larger academic trends. However, one would naturally expect the co-occurrences of words in titles to follow patterns, where words typical to research fields and subfields would often appear together. Our next goal is to see if this indeed happens, and assuming it does (yes, it does), use it to\n",
    "* identify what are the academic field distinctions that the co-occurrence graph holds.\n",
    "* analyse how the popularities of these different fields have varied.\n",
    "\n",
    "So we want to find groups of words that typically appear together. This could be called clustering, but in the context of networks/graphs like our co-occurrence graph, it usually goes by the name of community structure. There's plenty of research done on algorithms for identifying communities. I use below one quite well-known one, the [Louvain algorithm](https://arxiv.org/abs/0803.0476), which is a heuristic algorithm based on optimizing the modularity of the graph, i.e. minimizing the weight of edges connecting different communities and maximizing the weight of edges internal to communities. I tried a few other methods as well, most notably label-propagation and stochastic block models, but at least with the parameters that I tried they seemed to produce communities that were either very small or did not match well my human intuition of which words I would expect to be related."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are running this notebook yourself, you can safely go make a cup of tea at this point. The community finding takes around 5 minutes on my laptop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition = community.best_partition(cooc_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# community.best_partition returns a dictionary of {word: community_label}.\n",
    "# Lets turn that into a DataFrame and extract the lists of words belonging to each community.\n",
    "community_labels = pd.DataFrame(\n",
    "    {\"Label\": tuple(partition.values())},\n",
    "    index=partition.keys(),\n",
    "    dtype=\"category\",\n",
    ")\n",
    "communities = tuple(\n",
    "    map(tuple, community_labels.groupby(\"Label\").groups.values())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of communities: 15\n",
      "Numbers of words in the communities:\n",
      "[3630, 5807, 4115, 5020, 2974, 11, 4, 3, 2, 3, 5, 3, 2, 3, 2]\n",
      "Some example words from each community:\n",
      "('study', 'development', 'analysis', 'investigation', 'role', 'case', 'social', 'between', 'reference', 'use', 'behaviour', 'approach', 'theory', 'management', 'new', 'model', 'aspects', 'performance', 'education', 'factors', 'impact', 'evaluation', 'learning', 'english', 'health', 'policy', 'british', 'during', 'towards', 'special', 'influence', 'uk', 'change', 'practice', 'england', 'assessment', 'children', 'production', 'process', 'processes')\n",
      "\n",
      "('studies', 'using', 'systems', 'control', 'modelling', 'design', 'synthesis', 'system', 'properties', 'structure', 'application', 'effect', 'applications', 'high', 'models', 'based', 'methods', 'dynamics', 'structural', 'techniques', 'flow', 'data', 'experimental', 'networks', 'metal', 'power', 'interaction', 'problems', 'reactions', 'energy', 'processing', 'structures', 'materials', 'surface', 'related', 'compounds', 'water', 'measurement', 'optical', 'magnetic')\n",
      "\n",
      "('effects', 'human', 'characterisation', 'novel', 'molecular', 'cell', 'cells', 'growth', 'disease', 'regulation', 'function', 'interactions', 'protein', 'mechanisms', 'genetic', 'response', 'gene', 'cancer', 'expression', 'functional', 'potential', 'activity', 'metabolism', 'responses', 'proteins', 'identification', 'patients', 'complex', 'rat', 'acid', 'resistance', 'dna', 'associated', 'biological', 'changes', 'receptor', 'vitro', 'virus', 'type', 'signalling')\n",
      "\n",
      "('early', 'history', 'century', 'contemporary', 'late', 'art', 'modern', 'war', 'music', 'literature', 'representation', 'writing', 'fiction', 'historical', 'works', 'poetry', 'american', 'french', 'representations', 'interpretation', 'de', 'religious', 'john', 'thought', 'literary', 'greek', 'church', 'significance', 'period', 'theology', 'narrative', 'german', 'place', 'irish', 'philosophy', 'roman', 'christian', 'old', 'translation', 'france')\n",
      "\n",
      "('evolution', 'l', 'distribution', 'ecology', 'scotland', 'central', 'species', 'north', 'population', 'plant', 'biology', 'soil', 'variation', 'southern', 'region', 'marine', 'western', 'composition', 'sea', 'area', 'nitrogen', 'climate', 'plants', 'conservation', 'eastern', 'populations', 'diversity', 'soils', 'ecological', 'certain', 'morphology', 'temporal', 'landscape', 'forest', 'upper', 'lower', 'physiology', 'river', 'impacts', 'microbial')\n",
      "\n",
      "('bath', 'monooxygenase', 'oxidising', 'capsulatus', 'methylotrophic', 'obligate', 'methylococcus', 'methanotrophs', 'ob3b', 'methylosinus', 'trichosporium')\n",
      "\n",
      "('es', 'dar', 'frustration', 'salaam')\n",
      "\n",
      "('implantable', 'cardioverter', 'defibrillator')\n",
      "\n",
      "('script', 'cursive')\n",
      "\n",
      "('saccharopolyspora', 'erythraea', 'erythromycin')\n",
      "\n",
      "('described', 'suboptimal', 'fedbatch', 'exothermic', 'chosen')\n",
      "\n",
      "('faso', 'burkina', 'manoeuvre')\n",
      "\n",
      "('jumping', 'conclusions')\n",
      "\n",
      "('lightinduced', 'myxococcus', 'xanthus')\n",
      "\n",
      "('legionella', 'pneumophila')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of communities: {}\".format(len(communities)))\n",
    "print(\"Numbers of words in the communities:\")\n",
    "print([len(c) for c in communities])\n",
    "print(\"Some example words from each community:\")\n",
    "num_example_words = 40\n",
    "for c in communities:\n",
    "    print(c[:num_example_words], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So according to the Louvain algorithm, our co-occurrence graph has 19 communities, out of 5 are non-tiny. I'll just discard the 14 small ones as uninteresting (not every word needs to be in a community, recall that our graph only includes the ~20,000 most common words anyway), and focus on the 5 big ones.\n",
    "\n",
    "Gladly, they make quite good sense to human intuition. The first one has a lot of social sciences vocabulary in it, we'll call it Social; the second one is clearly hard sciences, maths, and engineering or just Science for short; the second is medicine and biochemistry, aka Biomed; the fourth one clearly Humanities; and the fifth has a theme of ecology and geography, and we'll call it Eco/Geo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point a note about stability of these communities is in order. I've been running the same community finding on graphs made with only subsets of the data set, leaving out some of the theses, and also tried including more or less words in the co-occurrence graph (remember we picked the arbitrary cutoff of only including words with at least 10 occurrences). The main communities fortunately appear quite stable under such perturbations. Including somewhat less words or less theses sometimes makes Humanities merge with Social sciences, but otherwise the communities stay roughly as they are. Going the other way, including more rare words in the graph may for instance sometimes cause particle physics to separate from the rest of Science. These kinds of granularity differences, of whether a field splits into a further subfields or merges with its academic neighbour, is quite natural, and doesn't in my view undermine the analysis in any significant way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, what we would like to do is assign to each word and thesis title a score or label, that would tell us roughly which field(s) it belongs in. The naive way would be to just label each word with its respective community, and count e.g. how many Humanities words appear in a given title. This doesn't seem quite fair though. Clearly some words are in some sense more \"deeply\" in each community. For instance the word \"theory\" gets grouped into social sciences, but obviously it occurs in other contexts as well, unlike the word \"policy\", which is pretty dead giveaway. To account for this effect, I give each word a score for how strongly they are connected to each community. This score starts out being 1 for the community the word belongs in and 0 for the others. I then add to it the total weight of edges connecting this word to words in a given community. So a word that is in the Humanities community and co-occurs mostly with other Humanities words gets a high Humanities-score, whereas a word that co-occurs with words from several different communities will have significant scores for all of them. Finally, we'll normalize the scores by the total sum within a field, so for instance the Social score of each word will be divided by the sum of Social scores of all words. This accounts for the fact that some fields include more words, and these words may be more strongly connected in our co-occurrence graph. We'll also multiply the resulting scores by 10,000, just to produce more human-readable numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, use some key words in each community to anchor the names of the fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_keywords = {\n",
    "    \"education\": \"Social\",\n",
    "    \"cell\": \"Biomed\",\n",
    "    \"magnetism\": \"Science\",\n",
    "    \"ecology\": \"Eco/Geo\",\n",
    "    \"philosophy\": \"Humanities\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the field scores, first give each word a score of 1.0 for the community it belongs in according to the Louvain classification. Drop all the small communities we don't care about, and name the columns of the DataFrame with the names given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordscores = pd.get_dummies(community_labels).astype(np.float_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns whose names are in field_keywords.\n",
    "for word, field in field_keywords.items():\n",
    "    current_label = wordscores.loc[word, :].idxmax()\n",
    "    wordscores.rename(columns={current_label: field}, inplace=True)\n",
    "# Drop all other columns.\n",
    "for c in wordscores.columns:\n",
    "    if not c in field_keywords.values():\n",
    "        wordscores.drop(c, axis=1, inplace=True)\n",
    "fieldnames = wordscores.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then add to the scores the weights of the edges connecting each word to words in different communities. This is easy to do with a matrix product with the adjacency matrix of the graph.\n",
    "\n",
    "To give an example, \"tunnel\" belongs to the Science community, so it starts out having a score of 1.0 for Science and 0.0 for the other fields. It has graph edges connecting it to many other Science words, that increase it Science score, but it also has an edge with weight 0.043 connecting it to \"boring\", which is an Eco/Geo word, and thus we add 0.043 to the Eco/Geo score of \"tunnel\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# So to get the scores we want, we need to take the matrix product of the\n",
    "# adjacency matrix of the co-occurrence graph with the current score matrix,\n",
    "# wordscores.values. The graph is way too big to build the whole adjacency\n",
    "# matrix as a dense matrix, but luckily NetworkX has us covered, with the\n",
    "# possibility of creating a scipy sparse matrix instead. However, the pandas\n",
    "# dot function for matrix products insists on converting everything to a dense\n",
    "# matrix to do the product, so we'll have to manually work with the\n",
    "# scipy.sparse matrix instead of a DataFrame. It's not too bad though.\n",
    "adjacency_matrix = nx.to_scipy_sparse_matrix(\n",
    "    cooc_graph, nodelist=wordscores.index, format=\"csr\"\n",
    ")\n",
    "for c in fieldnames:\n",
    "    column_vec = wordscores[c].to_numpy()\n",
    "    scores = adjacency_matrix.dot(column_vec)\n",
    "    scores_series = pd.Series(scores, index=wordscores.index)\n",
    "    wordscores[c] += scores_series\n",
    "del adjacency_matrix  # Release the memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, normalize the scores, and give all the words we did not include in our graph (the ones with fewer than 10 occurrences) a score of 0.0 for everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordscores *= 10000 / wordscores.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordscores = wordscores.reindex(allwords, fill_value=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check that the scoring system makes sense, let's print out the top words for each field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Words with highest scores for Social:\n",
      "               Social   Science    Biomed  Humanities   Eco/Geo\n",
      "case         6.716723  0.657182  0.334156    1.066387  0.816086\n",
      "education    6.695847  0.341190  0.215665    1.039075  0.269844\n",
      "experiences  6.526641  0.297143  0.569835    0.717744  0.179647\n",
      "school       6.483865  0.345614  0.290432    0.794999  0.252025\n",
      "students     6.430835  0.366024  0.215439    0.731259  0.177349\n",
      "health       6.255223  0.552894  1.061215    0.513001  0.491674\n",
      "\n",
      "Words with highest scores for Science:\n",
      "            Social   Science    Biomed  Humanities   Eco/Geo\n",
      "high      1.388744  4.503673  0.771652    0.200384  0.572297\n",
      "using     2.154928  4.486702  1.588313    0.248209  0.824461\n",
      "laser     0.341289  4.315170  0.425315    0.051416  0.172774\n",
      "flow      0.762912  4.211298  0.735748    0.097012  0.731297\n",
      "optical   0.421800  4.195425  0.417570    0.061817  0.184297\n",
      "detector  0.246862  4.161888  0.209603    0.112563  0.091157\n",
      "\n",
      "Words with highest scores for Biomed:\n",
      "              Social   Science    Biomed  Humanities   Eco/Geo\n",
      "cells       0.650580  1.191511  6.293296    0.069774  0.361524\n",
      "cell        0.833206  1.181138  6.118855    0.078857  0.510862\n",
      "receptor    0.426488  0.498565  6.042360    0.046470  0.219959\n",
      "expression  0.710838  0.540300  5.833130    0.240900  0.636784\n",
      "rat         0.531832  0.616799  5.777821    0.060370  0.347681\n",
      "gene        0.677382  0.613837  5.683110    0.080593  0.699415\n",
      "\n",
      "Words with highest scores for Humanities:\n",
      "             Social   Science    Biomed  Humanities   Eco/Geo\n",
      "century    2.330036  0.272086  0.175552    4.767523  0.522681\n",
      "centuries  0.952241  0.199807  0.106619    4.667784  0.468744\n",
      "john       0.952291  0.182106  0.112010    4.405140  0.244712\n",
      "theology   1.102907  0.175454  0.160677    4.338892  0.128931\n",
      "works      1.218011  0.289692  0.166877    4.167000  0.257032\n",
      "edition    0.474855  0.146911  0.161759    4.150754  0.175880\n",
      "\n",
      "Words with highest scores for Eco/Geo:\n",
      "            Social   Science    Biomed  Humanities   Eco/Geo\n",
      "l         0.809951  0.546924  1.395153    0.200012  7.947904\n",
      "basin     0.855223  0.469597  0.153280    0.437791  6.328881\n",
      "salmo     0.259562  0.236995  0.770210    0.146493  5.930395\n",
      "ecology   1.022744  0.302703  0.542362    0.271497  5.897572\n",
      "atlantic  0.697421  0.599104  0.540348    0.417421  5.887838\n",
      "brassica  0.166537  0.160085  0.699161    0.050950  5.778494\n"
     ]
    }
   ],
   "source": [
    "num_topwords = 6\n",
    "for c in fieldnames:\n",
    "    print(\"\\nWords with highest scores for {}:\".format(c))\n",
    "    print(wordscores.sort_values(c, ascending=False)[:num_topwords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a few funny ones, like \"L\" being the top word in Eco/Geo because it often stands for Carl Linnaeus, the founder of modern taxonomy, in scientific names of species, but overall the scores seem pretty reasonable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, computing the scores for each thesis title, by just taking the average of the scores of individual words in the title. This takes a minute or so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing title scores for Social.\n",
      "Computing title scores for Science.\n",
      "Computing title scores for Biomed.\n",
      "Computing title scores for Humanities.\n",
      "Computing title scores for Eco/Geo.\n"
     ]
    }
   ],
   "source": [
    "for c in fieldnames:\n",
    "    print(\"Computing title scores for {}.\".format(c))\n",
    "    # Interestingly the python sum function is significantly faster here than a\n",
    "    # sub-DataFrame.sum(). Probably due to allocation of temporaries.\n",
    "    df[c] = df[\"Words\"].apply(\n",
    "        lambda ws: 0.0  # Special case to avoid divison by zero.\n",
    "        if not ws\n",
    "        else sum(wordscores.loc[w, c] for w in ws) / len(ws)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trends in academic fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With each thesis scored on all the academic fields, it's time to see what this data tells us. Let's first see how the scores are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3941b17623304c53968b9e177333b455",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bins = 50  # For histograms\n",
    "maxscore = 5  # Max score to show on the plot\n",
    "\n",
    "\n",
    "def custom_light_palette(color):\n",
    "    \"\"\" A color palette a bit like seaborn.light_palette, but starts from pure\n",
    "    white and goes to darker shades.\n",
    "    \"\"\"\n",
    "    start = sns.set_hls_values(color, l=1.0)\n",
    "    end = sns.set_hls_values(color, l=0.1)\n",
    "    colors = [start, color, end]\n",
    "    return sns.blend_palette(colors, as_cmap=True)\n",
    "\n",
    "\n",
    "def hist2d(x, y, color, **kwargs):\n",
    "    cmap = custom_light_palette(color)\n",
    "    rng = [[0, maxscore]] * 2\n",
    "    plt.hist2d(x, y, bins=bins, range=rng, cmap=cmap, **kwargs)\n",
    "\n",
    "\n",
    "def hist(x, color, **kwargs):\n",
    "    rng = [0, maxscore]\n",
    "    plt.hist(x, lw=0.0, bins=bins, range=rng, **kwargs)\n",
    "\n",
    "\n",
    "# A color palette for printing the correlation numbers.\n",
    "corr_palette = sns.diverging_palette(\n",
    "    10, 220, s=99, l=50, center=\"dark\", as_cmap=True\n",
    ")\n",
    "\n",
    "\n",
    "def corr(x, y, color, **kwargs):\n",
    "    \"\"\" Given two Series, compute their correlation and produce a pyplot plot\n",
    "    that just has that number as text in the middle, and nothing else.\n",
    "    \"\"\"\n",
    "    corr = x.corr(y)\n",
    "    corr_str = \"{:.3f}\".format(corr)\n",
    "    color = corr_palette(corr + 0.5)\n",
    "    plt.plot()\n",
    "    ax = plt.gca()\n",
    "    plt.text(\n",
    "        0.5,\n",
    "        0.5,\n",
    "        corr_str,\n",
    "        horizontalalignment=\"center\",\n",
    "        verticalalignment=\"center\",\n",
    "        transform=ax.transAxes,\n",
    "        color=color,\n",
    "    )\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(False)\n",
    "    ax.tick_params(axis=\"both\", which=\"both\", length=0)\n",
    "\n",
    "\n",
    "sns.set_style(\"white\")  # We use a different style for this plot than most.\n",
    "g = sns.PairGrid(df[fieldnames], height=1.5)\n",
    "g.map_upper(corr)\n",
    "g.map_lower(hist2d)\n",
    "g.map_diag(hist)\n",
    "g.set(ylim=(0, maxscore), xlim=(0, maxscore))\n",
    "ticks = [0.0, maxscore / 2, maxscore]\n",
    "g.set(xticks=ticks, yticks=ticks)\n",
    "sns.set_style(\"darkgrid\")  # Back to the usual style."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here on the diagonal are histograms showing the distribution of theses over the score for each field. Above the diagonal, the single numbers shown there are correlation coefficients between the field scores. For instance since the second row of plots is for Science and the fourth column is for Humanities, the number at their intersection, around -0.5, is the correlation coefficient between Science and Humanities scores. Mirroring these correlation coefficients, below the diagonal are 2D histograms showing the distribution of theses over two different field scores.\n",
    "\n",
    "Looking first at the 1D histograms on the diagonal, note that the first peak in each of them is not the interesting part. Since I smudged the boundaries of fields by adding neighbouring edge weights when scoring words, most words have a non-zero score for all the fields. The first peaks then represent a sort of baseline value, of what's a usual score for a thesis that isn't really in that field. The interesting bits are the fat tails, the bulges, around scores of 2 and more. They represent the theses that score unusually high for a given field.\n",
    "\n",
    "The next observation is that the distributions for different fields are very different. This is natural, given that some (e.g. Eco/Geo) are much more specific than others (e.g. Science). This does mean though that, despite our best attempts to normalize the scores meaningfully, we probably shouldn't assign much meaning to statements like \"this thesis has a much higher Humanities than Social score\", but rather concentrate on correlations and trends that are independent of the overall scale of the scores.\n",
    "\n",
    "The correlations tell us a lot about how distinct the different fields are. The highest positive correlation is between Humanities and Social sciences, which isn't very surprising, and goes together with my observation that sometimes the two fields fuse in the community finding analysis if a smaller data set is used. They both are heavily anti-correlated with Science and Biomed, again matching intuitive expectation. The one thing that somewhat surprises me is that Science, Biomed, and Eco/Geo are virtually independent of each other: I would have guessed topics like chemistry and population biology to cause some mixing between them. This is good though, since it makes the 5-way scoring more meaningful, when the scores aren't predictable from each other. The 2D histograms below the diagonal provide a more detailed visual picture of what is summarised by the correlation coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being pretty happy with our scoring method, lets see what trends we can detect over time and institutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off, here are the mean scores of theses published in each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fc978083a184fb387f86378f352e763",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "meanscores_by_year = df.groupby([\"Year\"]).mean().fillna(0.0)\n",
    "# Prepare to a format that sns.FacetGrid likes.\n",
    "meanscores_by_year = (\n",
    "    meanscores_by_year.unstack()\n",
    "    .reset_index()\n",
    "    .rename(columns={0: \"Mean score\", \"level_0\": \"Field\"})\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    meanscores_by_year,\n",
    "    col=\"Field\",\n",
    "    col_wrap=2,\n",
    "    col_order=fieldnames,\n",
    "    sharex=True,\n",
    "    sharey=True,\n",
    "    height=2.0,\n",
    "    aspect=1.8,\n",
    ")\n",
    "g.map(plt.plot, \"Year\", \"Mean score\")\n",
    "g.set_titles(\"{col_name}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see at least three clear trends here. \n",
    "* Social sciences first decline in popularity until around 1970, after which they start bouncing back, and have been growing to this day. Humanities follow a more mellow development of roughly the same shape.\n",
    "* Contrasting with that, Science follows an almost opposite development, rising and then falling. Especially notable is the big bulge around the turning point, in the 60s.\n",
    "* Eco/Geo follows a pretty steady pattern until the 80s, after which it has settled into a decline, that may have leveled off as of late.\n",
    "\n",
    "The reversal of the Social/Humanities vs Science trend in the 60s coincides roughly with the explosion of PhD production from smaller institutions that we observed earlier, but I'm unsure as to how that connection should be interpreted, and whether these events truly are connected. Regardless, the V-shaped competing curves of soft vs hard fields is clearly the grand narrative in these plots. It would be very interesting to try to compare it to other societal and academic trends, such as gender distributions of PhD graduates, changes in academic funding in the UK, and data on students at earlier stages in their education, such as the popularity of different undergraduate majors and changes in primary and secondary education curricula. For the moment, though, this remains work-to-be-done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can observe academic trends for individual institutions. For instance, here are the same plots as above, but specific to some of the golden triangle universities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cff0dd0b18244058eca091d78471f9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "select_insts = [\n",
    "    \"University of Cambridge\",\n",
    "    \"Imperial College London\",\n",
    "    \"University of Oxford\",\n",
    "    \"London School of Economics and Political Science (University of London)\",\n",
    "]\n",
    "meanscores_by_yearandinst = (\n",
    "    df[df.Institution.apply(lambda x: x in select_insts)]\n",
    "    .groupby([\"Institution\", \"Year\"])\n",
    "    .mean()  # Average scores for each year and institution\n",
    "    .unstack(\"Institution\")\n",
    "    .fillna(0.0)\n",
    "    # Rolling Gaussian average with std=3.0.\n",
    "    .rolling(10, center=True, win_type=\"gaussian\")\n",
    "    .mean(std=3.0)\n",
    "    .dropna()  # Rolling average causes some NaNs at the ends.\n",
    "    # Prepare to a format that sns.FacetGrid likes.\n",
    "    .stack(level=[0, 1])\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_1\": \"Field\", 0: \"Mean score\"})\n",
    ")\n",
    "g = sns.FacetGrid(\n",
    "    meanscores_by_yearandinst,\n",
    "    col=\"Field\",\n",
    "    col_order=fieldnames,\n",
    "    hue=\"Institution\",\n",
    "    # We pick a specific order for the institutions so that with the default\n",
    "    # color scheme the colors roughly match the official colors of the\n",
    "    # universities (Oxford is blue, Cambridge is red).\n",
    "    hue_order=[\n",
    "        \"University of Oxford\",\n",
    "        \"Imperial College London\",\n",
    "        \"London School of Economics and Political Science (University of London)\",\n",
    "        \"University of Cambridge\",\n",
    "    ],\n",
    "    col_wrap=2,\n",
    "    sharex=True,\n",
    "    sharey=False,\n",
    "    height=2.0,\n",
    "    aspect=1.8,\n",
    ")\n",
    "g.map(plt.plot, \"Year\", \"Mean score\")\n",
    "g.set_titles(\"{col_name}\")\n",
    "g.add_legend()\n",
    "# Seaborn puts the legend in a silly place, so move it manually.\n",
    "g._legend.set_bbox_to_anchor((0.78, 0.2));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've chosen to smooth the curves out a bit by doing a Gaussian rolling average of the data over time. Since there aren't that many theses coming out from a single institution in a single year the plots would look quite noisy and hard to read otherwise.\n",
    "\n",
    "Hardly surprisingly, London School of Economics is heavy on humanities and social sciences, and consistently puts out PhD theses that score higher in those fields than other universities. Less obviously, the above plot also provides evidence for the often-heard view that of the Oxbridge two, Cambridge is the more science-heavy one, whereas Oxford leans towards humanities and social sciences. Imperial lives up to its reputation of being a science school first and foremost, but less expected is its huge growth in Biomed in the 90s and early 2000s. [Wikipedia](https://en.wikipedia.org/wiki/Imperial_College_London#20th_century) informs us that this is probably due to mergers with several medical institutions around that time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epilogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I must say that I'm surprised by how much mileage one could get from just thesis metadata, without even abstracts to work with. This whole project started out as a simple exercise in learning some basic pandas, but quickly grew way beyond my original intentions.\n",
    "\n",
    "One could keep exploring trends for individual words and institutions, but beyond that there are a few additional ideas that I have considered, but haven't gotten around to doing:\n",
    "* Look for thesis titles with abnormal word combinations. Perhaps take the average co-occurrence graph path length between the words in the title, and rank titles based on that. Outlier theses that respect no field boundaries? Yes, please!\n",
    "* Visualize the co-occurrence graph. The main problem here is that the graph is relatively densely connected. It has ~20,000 words in it, and thus some 400 million possible edges, out which, if I recall correctly, around 6% exist with a non-zero weight. I don't know much about graph visualization, but this sounds to me like a challenge. Visually seeing the words corresponding to different fields cluster would be quite satisfying though.\n",
    "* Dig deeper into community finding algorithms for networks, and see if I can get something other than the Louvain algorithm to work. I already briefly tried some stochastic block models and label propagation, but, not being an expert, my lack of success with them may be entirely due to user errors.\n",
    "* More generally, study the co-occurrence graph further. How small-world is it, what are average path lengths, etc.\n",
    "* Test out some hypotheses for word popularity. For instance, at one point I was considering the hypothesis that humanities and social sciences words have less peaked popularity profiles over time compared to science words. \n",
    "* Combine the data here with some university rankings, and see if I can predict the ranking of a university based on PhD theses coming out of it. Credit to Henri Seijo for proposing this.\n",
    "* Find other data sets that I could combine this analysis with. Especially interesting could be data on academic funding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test any of the above ideas, or otherwise build further on what I've done here, you more than welcome to. I would also be interested in hearing about it if you do so: markus@mhauru.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "15791a1337e34a09934985906e1adb90": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_9707be2c31b64f65af5212a969f953fd",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "23b8a8aaa4924119b6eb09f73f3eff56": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "2cff0dd0b18244058eca091d78471f9f": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_afceb379db184b3c8d78fddf439002b5",
       "toolbar": "IPY_MODEL_4bf5950668f547ea815f1b7cf90568f8",
       "toolbar_position": "left"
      }
     },
     "2f8e40c8a42b4437aaec7b49302e510e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3122e2c3222145619e445f62f87d4aac": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_64904de1d4964fc49877e2d14717db79",
       "toolbar": "IPY_MODEL_3f5a745a7d5249b181e77d8463f906d7",
       "toolbar_position": "left"
      }
     },
     "335c4a498eac438084cf30f67f2d20db": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_a778ab0181364d36aecdd2376325f4cf",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "39097085468a40d1a60273282e30a2dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "3941b17623304c53968b9e177333b455": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_8347596858fc4dddbdf82edc3385b629",
       "toolbar": "IPY_MODEL_d8594332995a400d97a94bf81d75a592",
       "toolbar_position": "left"
      }
     },
     "3f5a745a7d5249b181e77d8463f906d7": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_d2b4d146a36d4af7b0211f124b49e17f",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "4429b0ecf3b949cb828de8002ad0a306": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "45f812b369a6408abdd392947e5eaf25": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_c762810dad6447209424d237b0397b52",
       "toolbar": "IPY_MODEL_473c5dddb1a5430eb889242b07bb7961",
       "toolbar_position": "left"
      }
     },
     "473c5dddb1a5430eb889242b07bb7961": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_dd3ad17b8d92412da8d5c6c6b3f40d95",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "4bf5950668f547ea815f1b7cf90568f8": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_badc0e8d0bb64049b86f45fe65ca6529",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "5fc978083a184fb387f86378f352e763": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_ec54426fa61741f9823ff306a5b189c9",
       "toolbar": "IPY_MODEL_c1427ca42ad6426d8fb805df3d4cfbc6",
       "toolbar_position": "left"
      }
     },
     "64904de1d4964fc49877e2d14717db79": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "69a251dc1b7a4c61817297ee8c7dfe8c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "6a73458fbdf248b887b7f5bfa98ff41e": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_a3f8e04a15d04025ae1cd8eb3b78723d",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "6c4b357487ed4f819b2262fd6a16c43b": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_23b8a8aaa4924119b6eb09f73f3eff56",
       "toolbar": "IPY_MODEL_335c4a498eac438084cf30f67f2d20db",
       "toolbar_position": "left"
      }
     },
     "8347596858fc4dddbdf82edc3385b629": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "9707be2c31b64f65af5212a969f953fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a3f8e04a15d04025ae1cd8eb3b78723d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "a778ab0181364d36aecdd2376325f4cf": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "afceb379db184b3c8d78fddf439002b5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ba44fa454b88490e930bd53f95170d33": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_2f8e40c8a42b4437aaec7b49302e510e",
       "toolbar": "IPY_MODEL_15791a1337e34a09934985906e1adb90",
       "toolbar_position": "left"
      }
     },
     "badc0e8d0bb64049b86f45fe65ca6529": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "bb1690b57dae42248892f00c8977f974": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_c740301b5ed945d1911c51113aa329f6",
       "toolbar": "IPY_MODEL_ee9700987c0b48f7a5b34774ce1d0c64",
       "toolbar_position": "left"
      }
     },
     "c1427ca42ad6426d8fb805df3d4cfbc6": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_69a251dc1b7a4c61817297ee8c7dfe8c",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "c740301b5ed945d1911c51113aa329f6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "c762810dad6447209424d237b0397b52": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d2b4d146a36d4af7b0211f124b49e17f": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "d8594332995a400d97a94bf81d75a592": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_39097085468a40d1a60273282e30a2dd",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "d9aff2a046064d06aadcb23d56730e7e": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "MPLCanvasModel",
      "state": {
       "layout": "IPY_MODEL_4429b0ecf3b949cb828de8002ad0a306",
       "toolbar": "IPY_MODEL_6a73458fbdf248b887b7f5bfa98ff41e",
       "toolbar_position": "left"
      }
     },
     "dd3ad17b8d92412da8d5c6c6b3f40d95": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ec54426fa61741f9823ff306a5b189c9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     },
     "ee9700987c0b48f7a5b34774ce1d0c64": {
      "model_module": "jupyter-matplotlib",
      "model_module_version": "^0.5.0",
      "model_name": "ToolbarModel",
      "state": {
       "layout": "IPY_MODEL_ff9f9e1df3554de3b658499439559a07",
       "toolitems": [
        [
         "Home",
         "Reset original view",
         "home",
         "home"
        ],
        [
         "Back",
         "Back to previous view",
         "arrow-left",
         "back"
        ],
        [
         "Forward",
         "Forward to next view",
         "arrow-right",
         "forward"
        ],
        [
         "Pan",
         "Pan axes with left mouse, zoom with right",
         "arrows",
         "pan"
        ],
        [
         "Zoom",
         "Zoom to rectangle",
         "square-o",
         "zoom"
        ],
        [
         "Download",
         "Download plot",
         "floppy-o",
         "save_figure"
        ]
       ]
      }
     },
     "ff9f9e1df3554de3b658499439559a07": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {}
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
